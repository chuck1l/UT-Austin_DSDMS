{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "962d4c1a-c47a-48e2-8f66-defa02dd49a3",
   "metadata": {},
   "source": [
    "# Caching and Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d11075e-a2e7-412c-8261-02aa783ec3a9",
   "metadata": {},
   "source": [
    "## Spark Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b012b79f-53ad-4d08-8203-46903534c9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/25 03:41:23 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "22/04/25 03:41:23 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "22/04/25 03:41:23 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/04/25 03:41:23 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "app_name = \"week2_cache\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .config(\"spark.ui.port\",\"42229\")\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "## Change the working directory\n",
    "%cd /media"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aa6de2-2df7-4b0d-9b5b-e06a30f81648",
   "metadata": {},
   "source": [
    "## Caching\n",
    "\n",
    "So far we have worked with different types of RDDs and we know that Spark will not execute until an action forces execution, thanks to lazy evaluation. This process allows Spark to optimize the data distribution at execution. However, there are processess that are iterative in nature and this lazy evaluation can slow down performance. Enter caching. \n",
    "\n",
    "To avoid Spark computing the RDD each time, we can cache the data at executor level. The idea here is that the RDD will **persist** at executor level, so any networking is only done once.\n",
    "\n",
    "### Example - Gradient Descent\n",
    "\n",
    "Let's run an example. We will run gradient descent to come up with the regression coefficients.\n",
    "\n",
    "Linear Regression tackles the __prediction task__ by assuming that we can compute our output variable, $y$, using a linear combination of our input variables. That is we assume there exist a set of **weights**, such that for any input $\\mathbf{x}_j \\in \\mathbb{R}^{m+1}$:\n",
    "\n",
    "\\begin{equation}\\tag{1.1}\n",
    "y_j = \\displaystyle\\sum_{i=1}^{m+1}{w_i\\cdot x_{ji}}\n",
    "\\end{equation}\n",
    "\n",
    "In vector notation, this can be written:\n",
    "\n",
    "\\begin{equation}\n",
    "y_j = \\displaystyle{\\mathbf{w}^T\\mathbf{x}_{j}}\n",
    "\\end{equation}\n",
    "\n",
    "Linear Regression attempts to fit (i.e. **learn**) the best line (in 1 dimension) or hyperplane (in 2 or more dimensions) to the data.  In the case of **ordinary least squares (OLS)** linear regression, best fit is defined as minimizing the Euclidean distances of each point in the dataset to the line or hyperplane.  These distances are often referred to as **residuals**. \n",
    "\n",
    "There is a closed form solution for OLS, that you probably have seen in the past in your statistic class. However, at scale, matrix operations are too slow, so we use **Gradient Descent**. Without going too much into the technicals, GD is based on iterating through the loss function until we minimize it (**Optimization**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18e69a89-8062-4e6d-95ec-9264dc487918",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's define the two main functions: Compute the Loss and the Gradient Descent Update\n",
    "def OLSLoss(dataRDD, W):\n",
    "    \"\"\"\n",
    "    Compute mean squared error.\n",
    "    Args:\n",
    "        dataRDD - each record is a tuple of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    \"\"\"\n",
    "    # Add the intercept\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1]))\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = augmentedData.map(lambda x: (W.dot(x[0]) - x[1])**2).mean()\n",
    "    ################## (END) YOUR CODE ##################\n",
    "    return loss\n",
    "\n",
    "def GDUpdate(dataRDD, W, learningRate = 0.1):\n",
    "    \"\"\"\n",
    "    Perform one OLS gradient descent step/update.\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    Returns:\n",
    "        new_model - (array) updated coefficients, bias at index 0\n",
    "    \"\"\"\n",
    "    # Add the intercept\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1])).cache()\n",
    "    \n",
    "    # Compute the gradient\n",
    "    grad = augmentedData.map(lambda x: (W.dot(x[0]) - x[1])*x[0]).mean() * 2\n",
    "    new_model = W - learningRate * grad\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c449f645-078b-46fd-aa92-1451f09311e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up the data\n",
    "## Let's prepare the data\n",
    "# Number of features\n",
    "n = 50\n",
    "# Number of observations\n",
    "N = 10000\n",
    "\n",
    "np.random.seed(2022)\n",
    "X = np.random.uniform(size = (N,n))\n",
    "y = np.sum(X, axis=1) + np.random.normal(0,0.1,N)\n",
    "data = pd.DataFrame(np.vstack((X.T, y)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2b72646-5fde-447b-966c-eca43957ed1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "STEP: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/25 03:41:47 WARN org.apache.spark.scheduler.TaskSetManager: Stage 0 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/25 03:41:50 WARN org.apache.spark.scheduler.TaskSetManager: Stage 1 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.009826849449816691\n",
      "Model: [-0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "----------\n",
      "STEP: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/25 03:41:50 WARN org.apache.spark.scheduler.TaskSetManager: Stage 2 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/25 03:41:51 WARN org.apache.spark.scheduler.TaskSetManager: Stage 3 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.009825258503583697\n",
      "Model: [-0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "----------\n",
      "STEP: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/25 03:41:51 WARN org.apache.spark.scheduler.TaskSetManager: Stage 4 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/25 03:41:52 WARN org.apache.spark.scheduler.TaskSetManager: Stage 5 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.009823774724840903\n",
      "Model: [-0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "----------\n",
      "STEP: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/25 03:41:52 WARN org.apache.spark.scheduler.TaskSetManager: Stage 6 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/25 03:41:53 WARN org.apache.spark.scheduler.TaskSetManager: Stage 7 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.009822503314113504\n",
      "Model: [-0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "----------\n",
      "STEP: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/25 03:41:53 WARN org.apache.spark.scheduler.TaskSetManager: Stage 8 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/25 03:41:54 WARN org.apache.spark.scheduler.TaskSetManager: Stage 9 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.009821761400156535\n",
      "Model: [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.001, 1.0]\n",
      "CPU times: user 2.25 s, sys: 36.2 ms, total: 2.29 s\n",
      "Wall time: 12.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Run the model without caching\n",
    "  \n",
    "## RDD Creation\n",
    "PointsRDD = spark.createDataFrame(data)\\\n",
    "            .rdd.map(lambda Row: (Row[0:-1], Row[-1]))\n",
    "\n",
    "## Number of iterations\n",
    "nSteps = 5\n",
    "\n",
    "## Initial guess\n",
    "model = np.array([0]+[1]*n)\n",
    "\n",
    "for idx in range(nSteps):\n",
    "    print(\"----------\")\n",
    "    print(f\"STEP: {idx+1}\")\n",
    "    model = GDUpdate(PointsRDD, model)\n",
    "    loss = OLSLoss(PointsRDD, model)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    print(f\"Model: {[round(w,3) for w in model]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61726239-817d-4ba1-a1b8-9d7f109d44b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "STEP: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/25 03:42:01 WARN org.apache.spark.scheduler.TaskSetManager: Stage 10 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/25 03:42:01 WARN org.apache.spark.scheduler.TaskSetManager: Stage 11 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.009826849449816691\n",
      "Model: [-0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "----------\n",
      "STEP: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/25 03:42:01 WARN org.apache.spark.scheduler.TaskSetManager: Stage 12 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/25 03:42:02 WARN org.apache.spark.scheduler.TaskSetManager: Stage 13 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.009825258503583697\n",
      "Model: [-0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "----------\n",
      "STEP: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/25 03:42:02 WARN org.apache.spark.scheduler.TaskSetManager: Stage 14 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/25 03:42:02 WARN org.apache.spark.scheduler.TaskSetManager: Stage 15 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.009823774724840903\n",
      "Model: [-0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "----------\n",
      "STEP: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/25 03:42:02 WARN org.apache.spark.scheduler.TaskSetManager: Stage 16 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/25 03:42:03 WARN org.apache.spark.scheduler.TaskSetManager: Stage 17 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/25 03:42:03 WARN org.apache.spark.scheduler.TaskSetManager: Stage 18 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.009822503314113504\n",
      "Model: [-0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "----------\n",
      "STEP: 5\n",
      "Loss: 0.009821761400156535\n",
      "Model: [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.001, 1.0]\n",
      "CPU times: user 2.26 s, sys: 61.1 ms, total: 2.32 s\n",
      "Wall time: 4.98 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/25 03:42:03 WARN org.apache.spark.scheduler.TaskSetManager: Stage 19 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Same model, but now we cached the data\n",
    "## RDD Creation - Check that we cached the data at the end!\n",
    "PointsRDDcached = spark.createDataFrame(data)\\\n",
    "            .rdd.map(lambda Row: (Row[0:-1], Row[-1])).cache()\n",
    "\n",
    "## Number of iterations\n",
    "nSteps = 5\n",
    "\n",
    "## Initial guess\n",
    "model = np.array([0]+[1]*n)\n",
    "\n",
    "for idx in range(nSteps):\n",
    "    print(\"----------\")\n",
    "    print(f\"STEP: {idx+1}\")\n",
    "    model = GDUpdate(PointsRDDcached, model)\n",
    "    loss = OLSLoss(PointsRDDcached, model)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    print(f\"Model: {[round(w,3) for w in model]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27d74c9-1a98-40d1-9671-9d6faff32e32",
   "metadata": {},
   "source": [
    "## Broadcasting\n",
    "\n",
    "The other element that we need to introduce today are **broadcast variables**. Like accumulators before, broadcast variables allows us to share or send a file towards the executors for execution. Why is this important? Suppose you don't use them and have a large look up table or feature vector living in the driver. Each time that the executors need the file, needs to ask for it to the Driver, which will incurr in Network I/O, and slow down performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a025498-214e-4b74-ac01-771fed33121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's change our GD Update to include Broadcasting of the model\n",
    "def GDUpdateBroad(dataRDD, W, learningRate = 0.1):\n",
    "    \"\"\"\n",
    "    Perform one OLS gradient descent step/update.\n",
    "    Args:\n",
    "        dataRDD - records are tuples of (features_array, y)\n",
    "        W       - (array) model coefficients with bias at index 0\n",
    "    Returns:\n",
    "        new_model - (array) updated coefficients, bias at index 0\n",
    "    \"\"\"\n",
    "    # Add the intercept\n",
    "    augmentedData = dataRDD.map(lambda x: (np.append([1.0], x[0]), x[1])).cache()\n",
    "    \n",
    "    # Compute the gradient\n",
    "    W_broadcast = sc.broadcast(W)\n",
    "    grad = augmentedData.map(lambda x: (W_broadcast.value.dot(x[0]) - x[1])*x[0]).mean() * 2\n",
    "    new_model = W - learningRate * grad\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30d08c87-59a6-4ce3-99ae-e2a5c626e1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "STEP: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/25 03:42:16 WARN org.apache.spark.scheduler.TaskSetManager: Stage 20 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/25 03:42:16 WARN org.apache.spark.scheduler.TaskSetManager: Stage 21 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.009826849449816691\n",
      "Model: [-0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "----------\n",
      "STEP: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/25 03:42:16 WARN org.apache.spark.scheduler.TaskSetManager: Stage 22 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/25 03:42:17 WARN org.apache.spark.scheduler.TaskSetManager: Stage 23 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.009825258503583697\n",
      "Model: [-0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "----------\n",
      "STEP: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/25 03:42:17 WARN org.apache.spark.scheduler.TaskSetManager: Stage 24 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/25 03:42:18 WARN org.apache.spark.scheduler.TaskSetManager: Stage 25 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.009823774724840903\n",
      "Model: [-0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "----------\n",
      "STEP: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/25 03:42:18 WARN org.apache.spark.scheduler.TaskSetManager: Stage 26 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/25 03:42:18 WARN org.apache.spark.scheduler.TaskSetManager: Stage 27 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.009822503314113504\n",
      "Model: [-0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "----------\n",
      "STEP: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/25 03:42:19 WARN org.apache.spark.scheduler.TaskSetManager: Stage 28 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/25 03:42:19 WARN org.apache.spark.scheduler.TaskSetManager: Stage 29 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.009821761400156535\n",
      "Model: [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.001, 1.0]\n",
      "CPU times: user 2.32 s, sys: 56.7 ms, total: 2.37 s\n",
      "Wall time: 6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Let's run the same example as before, without caching but with broadcasting\n",
    "## RDD Creation\n",
    "PointsRDD = spark.createDataFrame(data)\\\n",
    "            .rdd.map(lambda Row: (Row[0:-1], Row[-1]))\n",
    "\n",
    "## Number of iterations\n",
    "nSteps = 5\n",
    "\n",
    "## Initial guess\n",
    "model = np.array([0]+[1]*n)\n",
    "\n",
    "for idx in range(nSteps):\n",
    "    print(\"----------\")\n",
    "    print(f\"STEP: {idx+1}\")\n",
    "    model = GDUpdateBroad(PointsRDD, model)\n",
    "    loss = OLSLoss(PointsRDD, model)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    print(f\"Model: {[round(w,3) for w in model]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "053c8cf9-f480-4b85-9747-ccc089649bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "STEP: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/25 03:42:27 WARN org.apache.spark.scheduler.TaskSetManager: Stage 30 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/25 03:42:27 WARN org.apache.spark.scheduler.TaskSetManager: Stage 31 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/25 03:42:27 WARN org.apache.spark.scheduler.TaskSetManager: Stage 32 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.009826849449816691\n",
      "Model: [-0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "----------\n",
      "STEP: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/25 03:42:28 WARN org.apache.spark.scheduler.TaskSetManager: Stage 33 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/25 03:42:28 WARN org.apache.spark.scheduler.TaskSetManager: Stage 34 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.009825258503583697\n",
      "Model: [-0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "----------\n",
      "STEP: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/25 03:42:28 WARN org.apache.spark.scheduler.TaskSetManager: Stage 35 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/25 03:42:28 WARN org.apache.spark.scheduler.TaskSetManager: Stage 36 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.009823774724840903\n",
      "Model: [-0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "----------\n",
      "STEP: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/25 03:42:28 WARN org.apache.spark.scheduler.TaskSetManager: Stage 37 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/04/25 03:42:29 WARN org.apache.spark.scheduler.TaskSetManager: Stage 38 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.009822503314113504\n",
      "Model: [-0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "----------\n",
      "STEP: 5\n",
      "Loss: 0.009821761400156535\n",
      "Model: [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.001, 1.0]\n",
      "CPU times: user 2.29 s, sys: 28.4 ms, total: 2.32 s\n",
      "Wall time: 4.65 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/25 03:42:29 WARN org.apache.spark.scheduler.TaskSetManager: Stage 39 contains a task of very large size (1390 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Let's now combine caching and broadcasting\n",
    "PointsRDDcached = spark.createDataFrame(data)\\\n",
    "            .rdd.map(lambda Row: (Row[0:-1], Row[-1])).cache()\n",
    "\n",
    "## Number of iterations\n",
    "nSteps = 5\n",
    "\n",
    "## Initial guess\n",
    "model = np.array([0]+[1]*n)\n",
    "\n",
    "for idx in range(nSteps):\n",
    "    print(\"----------\")\n",
    "    print(f\"STEP: {idx+1}\")\n",
    "    model = GDUpdateBroad(PointsRDDcached, model)\n",
    "    loss = OLSLoss(PointsRDDcached, model)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    print(f\"Model: {[round(w,3) for w in model]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547e3790-fe3c-44f4-bade-09e8a6a0279a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
