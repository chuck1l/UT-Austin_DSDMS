{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f27bcd74-fbc3-4baa-b398-8c168f8bc64f",
   "metadata": {},
   "source": [
    "# Loading and Saving Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbae6ee-ec2b-4610-99cb-0d2dfedd5a21",
   "metadata": {},
   "source": [
    "## Spark Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b219dc55-3547-461f-b380-e6c07db27359",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/19 14:06:06 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "22/04/19 14:06:06 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "22/04/19 14:06:06 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/04/19 14:06:06 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "app_name = \"week2_demo\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .config(\"spark.ui.port\",\"42229\")\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "## Change the working directory\n",
    "%cd /media"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60409b26-c8a1-4050-95d5-2d970f9dddb0",
   "metadata": {},
   "source": [
    "## File Formats\n",
    "\n",
    "Spark can support a variety of file formats. In the following table we summarize the most common supported file formats\n",
    "\n",
    "|  Format name | Structured  | Comments  |\n",
    "|:-----------:|:---:|:-----------------------------------------------------------|\n",
    "|  Text Files | No  | Plain text files. Records are assumed to be one per line  |\n",
    "|  JSON | Semi  |  Common text-based format. Semistructured. We will use python library json |\n",
    "|  CSV | Yes  |  Very common text-based format, often used in spreadsheet applications |\n",
    "\n",
    "In Week 3 we will cover the compression factor of CSV and compare it to two new data types that we will use in Streaming: parquet and avro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a8ac88-f430-4818-9155-decb10630822",
   "metadata": {},
   "source": [
    "### Text Files\n",
    "\n",
    "Quite simple to load from and save to with Spark. When we load a single text file as an RDD, each input line becomes a record of the RDD. We can also load multiple text files at the same time. In this case, it will be stored in a key/value pair RDD (Pair RDD), with the key being the name of the text file and the value the contents of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de97c416-eb39-4fc8-93a5-655f8879c981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\n",
      "Copying gs://great-learning-data-eng/alice.txt...\n",
      "/ [1 files][170.2 KiB/170.2 KiB]                                                \n",
      "Operation completed over 1 objects/170.2 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "## Download the Alice in Wonderland data\n",
    "### Store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]\n",
    "\n",
    "### Make the data directory\n",
    "!mkdir data\n",
    "\n",
    "### Download the data and store in the a data folder\n",
    "!gsutil cp gs://great-learning-data-eng/alice.txt data/\n",
    "ALICE_TXT = 'file:///media' + \"/data/alice.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dfece29-0d39-47f8-8390-df29d3a29aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data into an RDD\n",
    "aliceRDD = sc.textFile(ALICE_TXT)\n",
    "\n",
    "## Perform a word count\n",
    "result = aliceRDD.flatMap(lambda line: re.findall('[a-z]+', line.lower())) \\\n",
    "                 .map(lambda word: (word, 1)) \\\n",
    "                 .reduceByKey(lambda a, b: a + b)\\\n",
    "                 .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90584943-f05c-497a-a3ea-696d7c9e317f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('a', 695),\n",
       " ('abide', 2),\n",
       " ('able', 1),\n",
       " ('about', 102),\n",
       " ('above', 3),\n",
       " ('absence', 1),\n",
       " ('absurd', 2),\n",
       " ('accept', 1),\n",
       " ('acceptance', 1),\n",
       " ('accepted', 2)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get top 10 words (alphabetically)\n",
    "result.takeOrdered(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24773694-4490-4014-9b02-13e57267481f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1839),\n",
       " ('and', 942),\n",
       " ('to', 811),\n",
       " ('a', 695),\n",
       " ('of', 638),\n",
       " ('it', 610),\n",
       " ('she', 553),\n",
       " ('i', 546),\n",
       " ('you', 486),\n",
       " ('said', 462)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get top 10 words (by count)\n",
    "result.takeOrdered(10, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b788ea21-9f0a-4768-95be-f0507f3596cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's save as a text file\n",
    "outputPATH = 'file:///media' + \"/data/result\"\n",
    "result.saveAsTextFile(outputPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd4c7d6-6341-4927-a05a-17a6c6bf3a19",
   "metadata": {},
   "source": [
    "### JSON\n",
    "\n",
    "JSON files are a very popular semistructured data format, used by almost all web APIs and the web. Python has a very powerful built-in library. Let's take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2009204-ca01-47a2-8ac1-5b2cea662429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://great-learning-data-eng/annot_fpid.json...\n",
      "/ [1 files][  2.3 MiB/  2.3 MiB]                                                \n",
      "Operation completed over 1 objects/2.3 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "## Load the data\n",
    "!gsutil cp gs://great-learning-data-eng/annot_fpid.json data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "928f83af",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use Json Library to read the file\n",
    "json_path = \"file:///media/data/annot_fpid.json\"\n",
    "input = sc.textFile(json_path)\n",
    "data = input.map(lambda x: json.loads(x))\n",
    "\n",
    "## Define helper function to parse the lists\n",
    "def parse_list(x):\n",
    "    for i in x:\n",
    "        return (i, 1)\n",
    "\n",
    "## Calculate a frequency table\n",
    "freq_tableRDD = data.flatMap(lambda x: x.values())\\\n",
    "                    .map(parse_list)\\\n",
    "                    .reduceByKey(lambda x,y: x+y)\\\n",
    "                    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3fc482d-7fb0-412c-9204-ac7422d31ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('programming', 10005),\n",
       " ('javascript', 7082),\n",
       " ('ms_excel', 2130),\n",
       " ('sql', 2094),\n",
       " ('android', 1599),\n",
       " ('scala', 1538),\n",
       " ('php', 1453),\n",
       " ('puppet', 1354),\n",
       " ('powershell', 1315),\n",
       " ('r', 1200)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_tableRDD.takeOrdered(10, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f717dd-315c-45cc-92ed-6c122fec7020",
   "metadata": {},
   "source": [
    "### CSV\n",
    "\n",
    "Comma-Separated Values or CSVs are files that look very similar to an Excel Spreadsheet. Similar to JSON, we need to first load it as a text file and then we can process it. We can also use the `spark.read.csv` method. Let's showcase the latter (this will use a DataFrame, Week 3 will be where we discuss DataFrames in detail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "727b6d08-aba9-4d8e-b631-039295a854e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://great-learning-data-eng/lp_data.csv...\n",
      "/ [1 files][  8.7 KiB/  8.7 KiB]                                                \n",
      "Operation completed over 1 objects/8.7 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "## Load the data\n",
    "!gsutil cp gs://great-learning-data-eng/lp_data.csv data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3255cfa-c056-4616-8d47-375ef38e3baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Read the data\n",
    "csv_path = \"file:///media/data/lp_data.csv\"\n",
    "data_df = spark.read.csv(csv_path, header=True)\n",
    "type(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a54eb8ee-8a87-42fd-8ae8-2885d1d51e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(usage='1170.88', product_name=\"The Manager's Path, 1Ed\", delivery_code='SafBook', month='2017 / 10', pdb_ip_pub_date='2017-03-23'),\n",
       " Row(usage='21.13', product_name='XPath and XPointer, 1Ed', delivery_code='SafBook', month='2017 / 10', pdb_ip_pub_date='2002-07-31'),\n",
       " Row(usage='516.92', product_name='Learning Path: Advanced CSS & Sass', delivery_code='SafVideo', month='2017 / 10', pdb_ip_pub_date='2015-11-23'),\n",
       " Row(usage='2.06', product_name='Learning Path: 2017 Design Conference Viewer???s Choice, 1Ed', delivery_code='SafVideo', month='2017 / 10', pdb_ip_pub_date='2017-05-26'),\n",
       " Row(usage='1234.83', product_name=\"Learning Path: A Beginner's Guide to Architecting Big Data Applications, 1Ed\", delivery_code='SafVideo', month='2017 / 10', pdb_ip_pub_date='2016-12-13')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Transform the DataFrame into an RDD\n",
    "data_RDD = data_df.rdd\n",
    "data_RDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7628e46-f8ca-4407-bf6b-91ec3c25122c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32486.070000000007"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get total usage\n",
    "data_RDD.map(lambda x: float(x.usage))\\\n",
    "        .reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2917b9f9-ec01-405d-8d2d-324c52005213",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
