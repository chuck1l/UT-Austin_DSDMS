{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61933bbb-2721-40e4-a69b-93cc1f8804c0",
   "metadata": {},
   "source": [
    "# Accumulators\n",
    "\n",
    "## Spark Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47a8ecaf-c503-4853-93d8-afc1c493329a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/22 13:50:42 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "22/04/22 13:50:43 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "22/04/22 13:50:43 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/04/22 13:50:43 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "22/04/22 13:50:43 WARN org.apache.spark.util.Utils: Service 'SparkUI' could not bind on port 42229. Attempting port 42230.\n",
      "22/04/22 13:50:43 WARN org.apache.spark.util.Utils: Service 'SparkUI' could not bind on port 42230. Attempting port 42231.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "app_name = \"week2_demo\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .config(\"spark.ui.port\",\"42229\")\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "## Change the working directory\n",
    "%cd /media"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0531ed-2526-4735-a049-0d78164adfcf",
   "metadata": {},
   "source": [
    "## What are Accumulators?\n",
    "\n",
    "So far we have cover variables that live inside a functional closure. Each time that we call `map` on a Spark job, the data is distributed and each computation is done at each stage, independently from each other. Sometimes, we want to have a variable that is shared across the entire job. These variables are called **Accumulators**. Accumuators can aggregate variables at the stage level and bring the results back to the driver to be used for other purposes. \n",
    "\n",
    "They can be used for either counting events that are not related to the main Spark job, or for debugging purposes. Let's see some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dad8b24-4a6e-478a-8b22-1de90a708394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of blank lines in Alice is 951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Simple example, let's count the number of empty lines in the Alice RDD\n",
    "ALICE_TXT = 'file:///media' + \"/data/alice.txt\"\n",
    "aliceRDD = sc.textFile(ALICE_TXT)\n",
    "\n",
    "## Let's define our accumulator and then initialize. We can use accumulators for adding or multiplying\n",
    "blank_lines = sc.accumulator(0)\n",
    "\n",
    "## Let's create a function that counts the blank lines\n",
    "def count_blank(line):\n",
    "    if (line == \"\"):\n",
    "        blank_lines.add(1)\n",
    "\n",
    "## Let's run our accumulator using the foreach action\n",
    "aliceRDD.foreach(count_blank)\n",
    "\n",
    "## Let's print the result\n",
    "print(f\"Number of blank lines in Alice is {blank_lines}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d76a9bc1-1b08-4961-bdb2-741c0fa98783",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's run now what could go wrong with Accumulators\n",
    "## We'll use the grades data that we created before\n",
    "## Helper function to count the number of failing grades\n",
    "def parse_grades(line, accumulator):\n",
    "    \"\"\"Helper function to parse input & track failing grades.\"\"\"\n",
    "    student,course,grade = line.split(',')\n",
    "    grade = int(grade)\n",
    "    if grade < 60:\n",
    "        accumulator.add(1)\n",
    "    return(student,course, grade)\n",
    "\n",
    "## Accumulator to keep track of fails\n",
    "nFail = sc.accumulator(0)\n",
    "\n",
    "# Compute averages\n",
    "csv_path = 'file:///media' + \"/data/grades.csv\"\n",
    "gradesRDD = sc.textFile(csv_path)\\\n",
    "              .map(lambda x: parse_grades(x, nFail))\n",
    "\n",
    "studentAvgs = gradesRDD.map(lambda x: (x[0], (x[2], 1)))\\\n",
    "                       .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\\\n",
    "                       .mapValues(lambda x: x[0]/x[1])\n",
    "\n",
    "courseAvgs = gradesRDD.map(lambda x: (x[1], (x[2], 1)))\\\n",
    "                      .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1]))\\\n",
    "                      .mapValues(lambda x: x[0]/x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c06b0eb9-b4fe-4326-bd66-e644ee7b5952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== average by student =====\n",
      "[('10001', 92.5), ('10004', 90.0), ('10002', 70.0), ('10003', 60.0), ('10005', 72.5)]\n",
      "===== average by course =====\n",
      "[('102', 62.333333333333336), ('101', 87.0), ('103', 71.66666666666667)]\n",
      "===== number of failing grades awarded =====\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "## Take a look at the averages\n",
    "print(\"===== average by student =====\")\n",
    "print(studentAvgs.collect())\n",
    "print(\"===== average by course =====\")\n",
    "print(courseAvgs.collect())\n",
    "print(\"===== number of failing grades awarded =====\")\n",
    "print(nFail)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbcd6e6-dd07-41e9-a289-287ee028ab91",
   "metadata": {},
   "source": [
    "What went wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ffb0c1f-8fcb-4e7f-9226-48295b966245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== number of failing grades awarded =====\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "## Better way\n",
    "\n",
    "## Accumulator to keep track of fails\n",
    "nFail = sc.accumulator(0)\n",
    "\n",
    "def parse_grades(line):\n",
    "    \"\"\"Helper function to parse input & track failing grades.\"\"\"\n",
    "    student,course,grade = line.split(',')\n",
    "    grade = int(grade)\n",
    "    return(student,course, grade)\n",
    "\n",
    "def count_fails(line):\n",
    "    grade = line[2]\n",
    "    if grade < 60:\n",
    "        nFail.add(1)\n",
    "        \n",
    "## Now let's run our count\n",
    "csv_path = 'file:///media' + \"/data/grades.csv\"\n",
    "gradesRDD = sc.textFile(csv_path)\\\n",
    "              .map(lambda x: parse_grades(x))\n",
    "\n",
    "gradesRDD.foreach(count_fails)\n",
    "print(\"===== number of failing grades awarded =====\")\n",
    "print(nFail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8114c39f-0764-4556-b556-035b241d5da7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
