{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bc64311-c2fd-411c-b644-964b4296ca28",
   "metadata": {},
   "source": [
    "# Spark Shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8922d44-b3a8-47e9-8c5e-157e67f96cee",
   "metadata": {},
   "source": [
    "## Spark Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7117b058-f0b4-4149-944a-22a302698bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/20 15:05:52 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "22/04/20 15:05:52 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "22/04/20 15:05:52 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/04/20 15:05:52 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "22/04/20 15:05:53 WARN org.apache.spark.util.Utils: Service 'SparkUI' could not bind on port 42229. Attempting port 42230.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "app_name = \"week2_demo\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .config(\"spark.ui.port\",\"42229\")\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "## Change the working directory\n",
    "%cd /media"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2989c76a-8f04-41e3-a9d0-02aa0221003c",
   "metadata": {},
   "source": [
    "## What is the Spark Shuffle?\n",
    "\n",
    "Shuffling is an important mechanism of the Map Reduce framework and it is key that we understand when and how happens. Shuffling is when Spark redistribute the data across the different workers and executors (even machines if working on big enough cluster). For Wide transformations, Spark typically runs a shuffle to group elements from different stages and phases.\n",
    "But why is the shuffle expensive? Because it involves the following operations:\n",
    "\n",
    "* Disk Input/Output (I/O)\n",
    "* Involves serialization and deserialization of data\n",
    "* Network I/O\n",
    "\n",
    "## Steps of the Spark Shuffle\n",
    "\n",
    "Let's suppose we are running a `reduceByKey()` operation\n",
    "\n",
    "1. Spark will first run all map phases and tasks on all partitions and groups value for every key\n",
    "2. If the results of the map do not fit in memory, Spark will store the data on disk (Disk I/O)\n",
    "3. Spark shuffles the data across the different partitions\n",
    "4. It finally reduces tasks on each partition based on the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4d875a2-47ba-45b4-a78e-7c730bfcf9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions in the Alice RDD is 2\n"
     ]
    }
   ],
   "source": [
    "## Let's see some examples\n",
    "ALICE_TXT = 'file:///media' + \"/data/alice.txt\"\n",
    "aliceRDD = sc.textFile(ALICE_TXT)\n",
    "\n",
    "## Let's print the number of partitions\n",
    "print(f\"Number of partitions in the Alice RDD is {aliceRDD.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e05729f-1777-4783-b378-1e4d1af6324b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions in the result RDD is 2\n"
     ]
    }
   ],
   "source": [
    "## Now let's run the same word count that we did before\n",
    "## Perform a word count\n",
    "result = aliceRDD.flatMap(lambda line: re.findall('[a-z]+', line.lower())) \\\n",
    "                 .map(lambda word: (word, 1)) \\\n",
    "                 .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "## Let's print the number of partitions\n",
    "print(f\"Number of partitions in the result RDD is {result.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9040e8c3-2901-4c9b-984e-b4053905b9b9",
   "metadata": {},
   "source": [
    "## Shuffle Partition Size\n",
    "\n",
    "You will have control over the number of partitions when running the Shuffle, we will cover them later in this module. When you are dealing with smaller datasets, you should reduce the number of shuffle partitions to avoid having to shuffle a lot of tasks with almost no data.\n",
    "\n",
    "If the data is too large, having a small number of partitions can lead to memory errors, so you would want to increase it. \n",
    "\n",
    "Getting the sweet spot number is tricky, and should be done whenever you have any issues with the performance of your Spark Job. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7066313b-09dc-41ce-8ba4-fe05afa4b10c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
