{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ac2b5f4-31e8-44d7-9d2c-7de2d9990d81",
   "metadata": {},
   "source": [
    "# RDD Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7154307a-a435-4267-ab78-d7448ab8c233",
   "metadata": {},
   "source": [
    "## Spark Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebaa9900-6dde-42a2-9e9a-eba3070b01c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pl/pyzly9ns0tsg1m113w9rhxvw0000gn/T/ipykernel_12818/1530640427.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mapp_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"week1_demo\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "app_name = \"week1_demo\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d0ff95-caec-4074-9946-b33b84ae08db",
   "metadata": {},
   "source": [
    "## RDD Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a506b69c-331f-44e4-adc3-a3a782a5ada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a RDD\n",
    "my_RDD = sc.parallelize(range(1,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9d0b87e-fa53-4ce5-932a-14871ef8a703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## What happens when we try to display it?\n",
    "my_RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e50108ec-52a3-40fd-8e27-6ba67d9ff4ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Uh oh! What's happening? ... LAZY EVALUATION!. Let's force exectution\n",
    "my_RDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1837343a-1709-4a5d-961c-9f1bee12f6cf",
   "metadata": {},
   "source": [
    "## Narrow Transformations\n",
    "\n",
    "Let's start discussion Narrow Transformations. Narrow transformations run in parallel, thus they don't require any networking between nodes. A series of narrow transformations will be one stage. No need to shuffle.\n",
    "\n",
    "### Main Narrow Transformations\n",
    "- `map()`\n",
    "- `flatMap()`\n",
    "- `filter()`\n",
    "- `mapValues()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53e11e03-3e10-4cfa-9f7a-7a23c9693c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[3] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## LEt's use our previous RDD and expand the data structure to a tuple\n",
    "cubes_RDD = my_RDD.map(lambda x: (x, x**3))\n",
    "cubes_RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "badd19b2-ebd3-4877-9e23-dc51cf652a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (2, 8), (3, 27), (4, 64), (5, 125)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Same as before, Spark WILL NOT EXECUTE until needed (Lazy Evaluation!)\n",
    "cubes_RDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b094a678-afeb-4e23-ab4c-9c6af2845f57",
   "metadata": {},
   "source": [
    "How many RDDs do we have right now? 1 or 2?\n",
    "\n",
    "Let's check the Spark UI!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6e2158d-976e-4356-b2b9-a537a3a7519c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 125), (10, 1000), (15, 3375), (20, 8000), (25, 15625)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let's now filter our cubes, I want the cubes that are multiples of 5\n",
    "cubes_mult_5_RDD= cubes_RDD.filter(lambda x: x[1]%5 == 0)\n",
    "cubes_mult_5_RDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5019d03b-5c4f-420c-8b41-4090c33f068f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 125), (10, 1000), (15, 3375), (20, 8000), (25, 15625)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Great! One of the powers of Spark is to be able to concatenate transformations - Creating a Spark job!\n",
    "my_RDD = sc.parallelize(range(1,1000))\\\n",
    "        .map(lambda x: (x, x**3))\\\n",
    "        .filter(lambda x: x[1] % 5 == 0)\n",
    "\n",
    "my_RDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eb2e65-e85f-4fa9-a7fd-20ce729acc34",
   "metadata": {},
   "source": [
    "## Wide Transformations\n",
    "\n",
    "Wide Transformations (which are equivalent to the Reduce Phase of Map Reduce) are those that require the inputs for more than one stage, thus forcing a shuffle. \n",
    "\n",
    "### Main Wide Transformations\n",
    "- `reduceByKey()`\n",
    "- `aggregateByKey()`\n",
    "- `groupByKey()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "349bd719-db09-47f8-b2d9-53a5ac54325d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Item1', 1200), ('Item1', 850), ('Item2', 350), ('Item2', 400)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let's create a list of tuples of stock prices\n",
    "sales_amounts = [\n",
    "    ('Item1', 1200),\n",
    "    ('Item1', 850),\n",
    "    ('Item2',  350),\n",
    "    ('Item2', 400)\n",
    "]\n",
    "\n",
    "sales_RDD = sc.parallelize(sales_amounts)\n",
    "\n",
    "## collet() is an important action, similar to take(n), but it brings everything\n",
    "sales_RDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b718d6f0-f5fc-4f73-92d2-efcf43017f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Item2', 375.0), ('Item1', 1025.0)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let's compute our average sales per store\n",
    "sales_RDD.mapValues(lambda x: (x,1))\\\n",
    "        .reduceByKey(lambda x,y : (x[0] + y[0], x[1] + y[1]))\\\n",
    "        .mapValues(lambda x: x[0]/float(x[1]))\\\n",
    "        .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071b2169-7da6-4811-b08a-72056aae6e5a",
   "metadata": {},
   "source": [
    "# Actions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
