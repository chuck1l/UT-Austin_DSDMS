{"cells": [{"cell_type": "markdown", "metadata": {"id": "eXLGTmuQ-QmD"}, "source": "# PySpark Preprocessing (with COVID-19 Dataset)"}, {"cell_type": "code", "execution_count": 1, "metadata": {"id": "XJnWWlXj-QmI"}, "outputs": [], "source": "import pandas as pd\nimport numpy as np\nfrom datetime import date, timedelta, datetime\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pyspark \nfrom pyspark.sql import SparkSession, SQLContext\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.functions import * \nfrom pyspark.sql.types import * "}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n22/05/25 06:11:21 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n22/05/25 06:11:21 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n22/05/25 06:11:21 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n22/05/25 06:11:21 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"}], "source": "# Initiate the Spark Session\napp_name = \"covid19_india\"\nmaster = \"local[*]\"\nspark = SparkSession\\\n        .builder\\\n        .appName(app_name)\\\n        .master(master)\\\n        .config(\"spark.ui.port\",\"42229\")\\\n        .getOrCreate()\nsc = spark.sparkContext"}, {"cell_type": "code", "execution_count": 3, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 219}, "id": "U34xtkx9-QmJ", "outputId": "0effdc58-ca67-4fe2-e386-0dce86bb1661"}, "outputs": [{"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://new-test-m.c.dsdms-new-project.internal:42229\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>covid19_india</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7faec09dd220>"}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": "spark"}, {"cell_type": "markdown", "metadata": {"id": "Acc0jnc--QmK"}, "source": "## Data\n This dataset has information from the states and union territories of India at daily level.\n\n\nState level data comes from Ministry of Health & Family Welfare\n\n\nAcknowledgements\n\nThanks to Indian Ministry of Health & Family Welfare for making the data available to general public.\n\n\nThanks to covid19india.org for making the individual level details, testing details, vaccination details available to general public."}, {"cell_type": "markdown", "metadata": {"id": "B9Sojq8C-QmK"}, "source": "Data can be found in this kaggle URL https://www.kaggle.com/datasets/sudalairajkumar/covid19-in-india"}, {"cell_type": "markdown", "metadata": {"id": "LMnfSP9m-QmL"}, "source": "### 1. Basic Functions"}, {"cell_type": "markdown", "metadata": {"id": "KFXUaufS-QmL"}, "source": "#### [1] Load (Read) the data"}, {"cell_type": "code", "execution_count": 6, "metadata": {"id": "KNKnGDQ4-QmL"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Change the path according to your google cloud bucket\ncases = spark.read.load(\"gs://dataproc-staging-asia-east2-441991837520-q2hvd2t0/notebooks/jupyter/covid_19_india.csv\",\n                        format=\"csv\", \n                        sep=\",\", \n                        inferSchema=\"true\", \n                        header=\"true\")"}, {"cell_type": "code", "execution_count": 7, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "iYKNfdva-QmL", "outputId": "deb25dde-984b-4b72-df2b-9d494cbde637"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+----------+-------+--------------------+-----------------------+------------------------+-----+------+---------+\n|Sno|      Date|   Time|State/UnionTerritory|ConfirmedIndianNational|ConfirmedForeignNational|Cured|Deaths|Confirmed|\n+---+----------+-------+--------------------+-----------------------+------------------------+-----+------+---------+\n|  1|2020-01-30|6:00 PM|              Kerala|                      1|                       0|    0|     0|        1|\n|  2|2020-01-31|6:00 PM|              Kerala|                      1|                       0|    0|     0|        1|\n|  3|2020-02-01|6:00 PM|              Kerala|                      2|                       0|    0|     0|        2|\n|  4|2020-02-02|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n|  5|2020-02-03|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n|  6|2020-02-04|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n|  7|2020-02-05|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n|  8|2020-02-06|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n|  9|2020-02-07|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n| 10|2020-02-08|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n| 11|2020-02-09|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n| 12|2020-02-10|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n| 13|2020-02-11|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n| 14|2020-02-12|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n| 15|2020-02-13|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n| 16|2020-02-14|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n| 17|2020-02-15|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n| 18|2020-02-16|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n| 19|2020-02-17|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n| 20|2020-02-18|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n+---+----------+-------+--------------------+-----------------------+------------------------+-----+------+---------+\nonly showing top 20 rows\n\n"}], "source": "# First few rows in the file\ncases.show()"}, {"cell_type": "markdown", "metadata": {"id": "_5LsnvNx-QmM"}, "source": "It looks ok right now, but sometimes as we the number of columns increases, the formatting becomes not too great. I have noticed that the following trick helps in displaying in pandas format in my Jupyter Notebook. \n\nThe **.toPandas()** function converts a **Spark Dataframe** into a **Pandas Dataframe**, which is much easier to play with."}, {"cell_type": "code", "execution_count": 8, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 363}, "id": "5vxlJypi-QmM", "outputId": "77e73805-7075-46a0-9e56-30fe0d442308"}, "outputs": [{"data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sno</th>\n      <th>Date</th>\n      <th>Time</th>\n      <th>State/UnionTerritory</th>\n      <th>ConfirmedIndianNational</th>\n      <th>ConfirmedForeignNational</th>\n      <th>Cured</th>\n      <th>Deaths</th>\n      <th>Confirmed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2020-01-30</td>\n      <td>6:00 PM</td>\n      <td>Kerala</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>2020-01-31</td>\n      <td>6:00 PM</td>\n      <td>Kerala</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>2020-02-01</td>\n      <td>6:00 PM</td>\n      <td>Kerala</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>2020-02-02</td>\n      <td>6:00 PM</td>\n      <td>Kerala</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>2020-02-03</td>\n      <td>6:00 PM</td>\n      <td>Kerala</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>2020-02-04</td>\n      <td>6:00 PM</td>\n      <td>Kerala</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>2020-02-05</td>\n      <td>6:00 PM</td>\n      <td>Kerala</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>2020-02-06</td>\n      <td>6:00 PM</td>\n      <td>Kerala</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>2020-02-07</td>\n      <td>6:00 PM</td>\n      <td>Kerala</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>2020-02-08</td>\n      <td>6:00 PM</td>\n      <td>Kerala</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "   Sno        Date     Time State/UnionTerritory ConfirmedIndianNational  \\\n0    1  2020-01-30  6:00 PM               Kerala                       1   \n1    2  2020-01-31  6:00 PM               Kerala                       1   \n2    3  2020-02-01  6:00 PM               Kerala                       2   \n3    4  2020-02-02  6:00 PM               Kerala                       3   \n4    5  2020-02-03  6:00 PM               Kerala                       3   \n5    6  2020-02-04  6:00 PM               Kerala                       3   \n6    7  2020-02-05  6:00 PM               Kerala                       3   \n7    8  2020-02-06  6:00 PM               Kerala                       3   \n8    9  2020-02-07  6:00 PM               Kerala                       3   \n9   10  2020-02-08  6:00 PM               Kerala                       3   \n\n  ConfirmedForeignNational  Cured  Deaths  Confirmed  \n0                        0      0       0          1  \n1                        0      0       0          1  \n2                        0      0       0          2  \n3                        0      0       0          3  \n4                        0      0       0          3  \n5                        0      0       0          3  \n6                        0      0       0          3  \n7                        0      0       0          3  \n8                        0      0       0          3  \n9                        0      0       0          3  "}, "execution_count": 8, "metadata": {}, "output_type": "execute_result"}], "source": "cases.limit(10).toPandas()"}, {"cell_type": "markdown", "metadata": {"id": "MQl6dk2V-QmM"}, "source": "#### [2] Change Column Names"}, {"cell_type": "markdown", "metadata": {"id": "SyyNdFG5-QmM"}, "source": "To change a single column,"}, {"cell_type": "code", "execution_count": 9, "metadata": {"id": "7KC5nfvm-QmM"}, "outputs": [], "source": "cases = cases.withColumnRenamed(\"ConfirmedIndianNational\",\"Confirmed_Indian_National\")"}, {"cell_type": "markdown", "metadata": {"id": "GsielLIN-QmN"}, "source": "To change all columns,"}, {"cell_type": "code", "execution_count": 10, "metadata": {"id": "t5NLdwMA-QmN"}, "outputs": [], "source": "cases = cases.toDF(*['Sno', 'Date', 'Time', 'State/UnionTerritory', 'ConfirmedIndianNational', 'ConfirmedForeignNational',\n       'Cured','Deaths', 'Confirmed'])"}, {"cell_type": "code", "execution_count": 11, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "_5poW82n-QmN", "outputId": "916e57fa-4aa8-4c5e-c0f8-8951b8716a76"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+----------+-------+--------------------+-----------------------+------------------------+-----+------+---------+\n|Sno|      Date|   Time|State/UnionTerritory|ConfirmedIndianNational|ConfirmedForeignNational|Cured|Deaths|Confirmed|\n+---+----------+-------+--------------------+-----------------------+------------------------+-----+------+---------+\n|  1|2020-01-30|6:00 PM|              Kerala|                      1|                       0|    0|     0|        1|\n|  2|2020-01-31|6:00 PM|              Kerala|                      1|                       0|    0|     0|        1|\n|  3|2020-02-01|6:00 PM|              Kerala|                      2|                       0|    0|     0|        2|\n|  4|2020-02-02|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n|  5|2020-02-03|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n|  6|2020-02-04|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n|  7|2020-02-05|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n|  8|2020-02-06|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n|  9|2020-02-07|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n| 10|2020-02-08|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n| 11|2020-02-09|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n| 12|2020-02-10|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n| 13|2020-02-11|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n| 14|2020-02-12|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n| 15|2020-02-13|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n| 16|2020-02-14|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n| 17|2020-02-15|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n| 18|2020-02-16|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n| 19|2020-02-17|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n| 20|2020-02-18|6:00 PM|              Kerala|                      3|                       0|    0|     0|        3|\n+---+----------+-------+--------------------+-----------------------+------------------------+-----+------+---------+\nonly showing top 20 rows\n\n"}], "source": "cases.show()"}, {"cell_type": "markdown", "metadata": {"id": "6BsIlike-QmN"}, "source": "#### [3] Change Column Names"}, {"cell_type": "markdown", "metadata": {"id": "qGpRwZgy-QmN"}, "source": "We can select a subset of columns using the **select** "}, {"cell_type": "code", "execution_count": 12, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "Qu4xoaZ--QmN", "outputId": "0daa03a6-6566-4cc9-ca73-d4db9357da12"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+-------+--------------------+------+\n|      Date|   Time|State/UnionTerritory|Deaths|\n+----------+-------+--------------------+------+\n|2020-01-30|6:00 PM|              Kerala|     0|\n|2020-01-31|6:00 PM|              Kerala|     0|\n|2020-02-01|6:00 PM|              Kerala|     0|\n|2020-02-02|6:00 PM|              Kerala|     0|\n|2020-02-03|6:00 PM|              Kerala|     0|\n|2020-02-04|6:00 PM|              Kerala|     0|\n|2020-02-05|6:00 PM|              Kerala|     0|\n|2020-02-06|6:00 PM|              Kerala|     0|\n|2020-02-07|6:00 PM|              Kerala|     0|\n|2020-02-08|6:00 PM|              Kerala|     0|\n|2020-02-09|6:00 PM|              Kerala|     0|\n|2020-02-10|6:00 PM|              Kerala|     0|\n|2020-02-11|6:00 PM|              Kerala|     0|\n|2020-02-12|6:00 PM|              Kerala|     0|\n|2020-02-13|6:00 PM|              Kerala|     0|\n|2020-02-14|6:00 PM|              Kerala|     0|\n|2020-02-15|6:00 PM|              Kerala|     0|\n|2020-02-16|6:00 PM|              Kerala|     0|\n|2020-02-17|6:00 PM|              Kerala|     0|\n|2020-02-18|6:00 PM|              Kerala|     0|\n+----------+-------+--------------------+------+\nonly showing top 20 rows\n\n"}], "source": "cases = cases.select('Date','Time','State/UnionTerritory','Deaths')\ncases.show()"}, {"cell_type": "markdown", "metadata": {"id": "PwLTn5q8-QmN"}, "source": "#### [4] Sort by Column"}, {"cell_type": "code", "execution_count": 13, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "ByjUtdqY-QmO", "outputId": "0f6b3396-8e49-4e0a-d055-3f139c979798"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+-------+--------------------+------+\n|      Date|   Time|State/UnionTerritory|Deaths|\n+----------+-------+--------------------+------+\n|2020-03-29|7:30 PM|              Kerala|     1|\n|2020-03-30|9:30 PM|               Delhi|     2|\n|2020-03-29|7:30 PM|              Ladakh|     0|\n|2020-03-29|7:30 PM|      Andhra Pradesh|     0|\n|2020-03-29|7:30 PM|      Madhya Pradesh|     2|\n|2020-03-29|7:30 PM|               Bihar|     1|\n|2020-03-29|7:30 PM|         Maharashtra|     6|\n|2020-03-29|7:30 PM|        Chhattisgarh|     0|\n|2020-03-29|7:30 PM|             Manipur|     0|\n|2020-03-29|7:30 PM|                 Goa|     0|\n|2020-03-29|7:30 PM|             Mizoram|     0|\n|2020-03-29|7:30 PM|             Haryana|     0|\n|2020-03-29|7:30 PM|              Odisha|     0|\n|2020-03-29|7:30 PM|   Jammu and Kashmir|     2|\n|2020-03-29|7:30 PM|          Puducherry|     0|\n|2020-03-29|7:30 PM|           Telengana|     1|\n|2020-03-30|9:30 PM|        Chhattisgarh|     0|\n|2020-03-29|7:30 PM|          Chandigarh|     0|\n|2020-03-29|7:30 PM|               Delhi|     2|\n|2020-03-29|7:30 PM|             Gujarat|     5|\n+----------+-------+--------------------+------+\nonly showing top 20 rows\n\n"}], "source": "# Simple sort\ncases.sort(\"ConfirmedIndianNational\").show()"}, {"cell_type": "code", "execution_count": 14, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "Vk4P6bQQ-QmO", "outputId": "97bdf437-00c4-4c97-aeff-2046f316ac15"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+-------+--------------------+------+\n|      Date|   Time|State/UnionTerritory|Deaths|\n+----------+-------+--------------------+------+\n|2020-03-25|6:00 PM|         West Bengal|     1|\n|2020-03-18|6:00 PM|               Delhi|     1|\n|2020-03-28|6:00 PM|               Bihar|     1|\n|2020-03-24|6:00 PM|         West Bengal|     1|\n|2020-03-09|6:00 PM|              Kerala|     0|\n|2020-03-11|6:00 PM|       Uttar Pradesh|     0|\n|2020-03-25|6:00 PM|      Andhra Pradesh|     0|\n|2020-03-24|6:00 PM|              Kerala|     0|\n|2020-03-24|6:00 PM|         Maharashtra|     2|\n|2020-03-18|6:00 PM|              Ladakh|     0|\n|2020-03-19|6:00 PM|              Ladakh|     0|\n|2020-03-20|6:00 PM|           Telengana|     0|\n|2020-03-28|6:00 PM|          Chandigarh|     0|\n|2020-03-08|6:00 PM|              Kerala|     0|\n|2020-03-24|6:00 PM|      Andhra Pradesh|     0|\n|2020-03-17|6:00 PM|               Delhi|     1|\n|2020-03-23|6:00 PM|         Maharashtra|     2|\n|2020-03-08|6:00 PM|       Uttar Pradesh|     0|\n|2020-03-05|6:00 PM|       Uttar Pradesh|     0|\n|2020-03-07|6:00 PM|       Uttar Pradesh|     0|\n+----------+-------+--------------------+------+\nonly showing top 20 rows\n\n"}], "source": "# Descending Sort\nfrom pyspark.sql import functions as F\n\ncases.sort(F.desc(\"ConfirmedIndianNational\")).show()"}, {"cell_type": "markdown", "metadata": {"id": "JE4WjJWT-QmO"}, "source": "#### [5] Change Column Type"}, {"cell_type": "code", "execution_count": 15, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "CzPFagiCJLif", "outputId": "f65472b3-7bab-4246-f7d7-0e5113a8f14f"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+-------+--------------------+------+\n|      Date|   Time|State/UnionTerritory|Deaths|\n+----------+-------+--------------------+------+\n|2020-01-30|6:00 PM|              Kerala|     0|\n|2020-01-31|6:00 PM|              Kerala|     0|\n|2020-02-01|6:00 PM|              Kerala|     0|\n|2020-02-02|6:00 PM|              Kerala|     0|\n|2020-02-03|6:00 PM|              Kerala|     0|\n|2020-02-04|6:00 PM|              Kerala|     0|\n|2020-02-05|6:00 PM|              Kerala|     0|\n|2020-02-06|6:00 PM|              Kerala|     0|\n|2020-02-07|6:00 PM|              Kerala|     0|\n|2020-02-08|6:00 PM|              Kerala|     0|\n|2020-02-09|6:00 PM|              Kerala|     0|\n|2020-02-10|6:00 PM|              Kerala|     0|\n|2020-02-11|6:00 PM|              Kerala|     0|\n|2020-02-12|6:00 PM|              Kerala|     0|\n|2020-02-13|6:00 PM|              Kerala|     0|\n|2020-02-14|6:00 PM|              Kerala|     0|\n|2020-02-15|6:00 PM|              Kerala|     0|\n|2020-02-16|6:00 PM|              Kerala|     0|\n|2020-02-17|6:00 PM|              Kerala|     0|\n|2020-02-18|6:00 PM|              Kerala|     0|\n+----------+-------+--------------------+------+\nonly showing top 20 rows\n\n"}], "source": "cases.show()"}, {"cell_type": "code", "execution_count": 16, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "XC6ztuvC-QmO", "outputId": "3df35c71-f0d1-4401-961b-a4a2e683b548"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+-------+--------------------+------+\n|      Date|   Time|State/UnionTerritory|Deaths|\n+----------+-------+--------------------+------+\n|2020-01-30|6:00 PM|              Kerala|     0|\n|2020-01-31|6:00 PM|              Kerala|     0|\n|2020-02-01|6:00 PM|              Kerala|     0|\n|2020-02-02|6:00 PM|              Kerala|     0|\n|2020-02-03|6:00 PM|              Kerala|     0|\n|2020-02-04|6:00 PM|              Kerala|     0|\n|2020-02-05|6:00 PM|              Kerala|     0|\n|2020-02-06|6:00 PM|              Kerala|     0|\n|2020-02-07|6:00 PM|              Kerala|     0|\n|2020-02-08|6:00 PM|              Kerala|     0|\n|2020-02-09|6:00 PM|              Kerala|     0|\n|2020-02-10|6:00 PM|              Kerala|     0|\n|2020-02-11|6:00 PM|              Kerala|     0|\n|2020-02-12|6:00 PM|              Kerala|     0|\n|2020-02-13|6:00 PM|              Kerala|     0|\n|2020-02-14|6:00 PM|              Kerala|     0|\n|2020-02-15|6:00 PM|              Kerala|     0|\n|2020-02-16|6:00 PM|              Kerala|     0|\n|2020-02-17|6:00 PM|              Kerala|     0|\n|2020-02-18|6:00 PM|              Kerala|     0|\n+----------+-------+--------------------+------+\nonly showing top 20 rows\n\n"}], "source": "from pyspark.sql.types import DoubleType, IntegerType, StringType\n\ncases = cases.withColumn('Deaths', F.col('Deaths').cast(IntegerType()))\ncases = cases.withColumn('State/UnionTerritory', F.col('State/UnionTerritory').cast(StringType()))\n\ncases.show()"}, {"cell_type": "markdown", "metadata": {"id": "Utdz_JI1-QmO"}, "source": "#### [6] Filter "}, {"cell_type": "markdown", "metadata": {"id": "hgbjD5z6-QmO"}, "source": "We can filter a data frame using multiple conditions using AND(&), OR(|) and NOT(~) conditions. For example, we may want to find out all the different infection_case in Daegu with more than 10 confirmed cases."}, {"cell_type": "code", "execution_count": 17, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "MFsWSqYw-QmO", "outputId": "a22686ae-db71-407c-e86b-dec596472eae"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+-------+--------------------+------+\n|      Date|   Time|State/UnionTerritory|Deaths|\n+----------+-------+--------------------+------+\n|2020-04-10|5:00 PM|               Delhi|    13|\n|2020-04-11|5:00 PM|               Delhi|    14|\n|2020-04-12|5:00 PM|               Delhi|    19|\n|2020-04-13|5:00 PM|               Delhi|    24|\n|2020-04-14|5:00 PM|               Delhi|    28|\n|2020-04-15|5:00 PM|               Delhi|    30|\n|2020-04-16|5:00 PM|               Delhi|    32|\n|2020-04-17|5:00 PM|               Delhi|    38|\n|2020-04-18|5:00 PM|               Delhi|    42|\n|2020-04-19|5:00 PM|               Delhi|    43|\n|2020-04-20|5:00 PM|               Delhi|    45|\n|2020-04-21|5:00 PM|               Delhi|    47|\n|2020-04-22|5:00 PM|               Delhi|    47|\n|2020-04-23|5:00 PM|               Delhi|    48|\n|2020-04-24|5:00 PM|               Delhi|    50|\n|2020-04-25|5:00 PM|               Delhi|    53|\n|2020-04-26|5:00 PM|               Delhi|    54|\n|2020-04-27|5:00 PM|               Delhi|    54|\n|2020-04-28|5:00 PM|               Delhi|    54|\n|2020-04-29|5:00 PM|               Delhi|    54|\n+----------+-------+--------------------+------+\nonly showing top 20 rows\n\n"}], "source": "cases.filter((cases.Deaths>10) & (cases[\"State/UnionTerritory\"]=='Delhi')).show()"}, {"cell_type": "markdown", "metadata": {"id": "8aOwC0n3-QmP"}, "source": "#### [7] GroupBy"}, {"cell_type": "code", "execution_count": 18, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "bVF2lTkE-QmP", "outputId": "88dae5a0-667d-4f9a-e931-59ca06784d43"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+--------------------+-----------+-----------+\n|      Date|State/UnionTerritory|sum(Deaths)|max(Deaths)|\n+----------+--------------------+-----------+-----------+\n|2020-02-22|              Kerala|          0|          0|\n|2020-03-25|             Manipur|          0|          0|\n|2020-03-31|         Maharashtra|          9|          9|\n|2020-04-06|             Gujarat|         12|         12|\n|2020-04-13|               Delhi|         24|         24|\n|2020-05-05|             Tripura|          0|          0|\n|2020-05-27|              Odisha|          7|          7|\n|2020-06-03|         West Bengal|        335|        335|\n|2020-06-07|              Kerala|         15|         15|\n|2020-06-14|          Tamil Nadu|        397|        397|\n|2020-06-27|   Arunachal Pradesh|          1|          1|\n|2020-06-29|          Puducherry|         10|         10|\n|2020-06-29|Cases being reass...|          0|          0|\n|2020-07-14|             Manipur|          0|          0|\n|2020-07-16|Andaman and Nicob...|          0|          0|\n|2020-07-16|                 Goa|         18|         18|\n|2020-07-18|Dadra and Nagar H...|          2|          2|\n|2020-07-26|         Maharashtra|      13389|      13389|\n|2020-07-26|         West Bengal|       1332|       1332|\n|2020-08-06|              Sikkim|          1|          1|\n+----------+--------------------+-----------+-----------+\nonly showing top 20 rows\n\n"}], "source": "from pyspark.sql import functions as F\n\ncases.groupBy([\"Date\",\"State/UnionTerritory\"]).agg(F.sum(\"Deaths\") ,F.max(\"Deaths\")).show()"}, {"cell_type": "markdown", "metadata": {"id": "tX9NDqPg-QmP"}, "source": "Or if we don\u2019t like the new column names, we can use the **alias** keyword to rename columns in the agg command itself."}, {"cell_type": "code", "execution_count": 22, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "_h5f1Dy2-QmP", "outputId": "e5fbe8a9-47d9-4ac7-8d1d-c4aab6502419"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+--------------------+------------+----------+\n|      Date|State/UnionTerritory|Total_Deaths|Max Deaths|\n+----------+--------------------+------------+----------+\n|2020-02-22|              Kerala|           0|         0|\n|2020-03-25|             Manipur|           0|         0|\n|2020-03-31|         Maharashtra|           9|         9|\n|2020-04-06|             Gujarat|          12|        12|\n|2020-04-13|               Delhi|          24|        24|\n|2020-05-05|             Tripura|           0|         0|\n|2020-05-27|              Odisha|           7|         7|\n|2020-06-03|         West Bengal|         335|       335|\n|2020-06-07|              Kerala|          15|        15|\n|2020-06-14|          Tamil Nadu|         397|       397|\n|2020-06-27|   Arunachal Pradesh|           1|         1|\n|2020-06-29|          Puducherry|          10|        10|\n|2020-06-29|Cases being reass...|           0|         0|\n|2020-07-14|             Manipur|           0|         0|\n|2020-07-16|Andaman and Nicob...|           0|         0|\n|2020-07-16|                 Goa|          18|        18|\n|2020-07-18|Dadra and Nagar H...|           2|         2|\n|2020-07-26|         Maharashtra|       13389|     13389|\n|2020-07-26|         West Bengal|        1332|      1332|\n|2020-08-06|              Sikkim|           1|         1|\n+----------+--------------------+------------+----------+\nonly showing top 20 rows\n\n"}], "source": "cases.groupBy([\"Date\",\"State/UnionTerritory\"]).agg(\n    F.sum(\"Deaths\").alias(\"Total_Deaths\"),\\\n    F.max(\"Deaths\").alias(\"Max Deaths\")\\\n    ).show()"}, {"cell_type": "markdown", "metadata": {"id": "WD2EbFkn-QmP"}, "source": "#### [8] Joins"}, {"cell_type": "markdown", "metadata": {"id": "pgCaRNNU-QmQ"}, "source": "Here, We will go with the region file which contains region information such as elementary_school_count, elderly_population_ratio, etc."}, {"cell_type": "code", "execution_count": 23, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 363}, "id": "Qg6jBVMm-QmQ", "outputId": "c55b4ec8-6388-4bd3-f387-3ac092feaabc"}, "outputs": [{"data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>State</th>\n      <th>TotalSamples</th>\n      <th>Negative</th>\n      <th>Positive</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2020-04-17</td>\n      <td>Andaman and Nicobar Islands</td>\n      <td>1403.0</td>\n      <td>1210</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2020-04-24</td>\n      <td>Andaman and Nicobar Islands</td>\n      <td>2679.0</td>\n      <td>None</td>\n      <td>27.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2020-04-27</td>\n      <td>Andaman and Nicobar Islands</td>\n      <td>2848.0</td>\n      <td>None</td>\n      <td>33.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2020-05-01</td>\n      <td>Andaman and Nicobar Islands</td>\n      <td>3754.0</td>\n      <td>None</td>\n      <td>33.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2020-05-16</td>\n      <td>Andaman and Nicobar Islands</td>\n      <td>6677.0</td>\n      <td>None</td>\n      <td>33.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2020-05-19</td>\n      <td>Andaman and Nicobar Islands</td>\n      <td>6965.0</td>\n      <td>None</td>\n      <td>33.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2020-05-20</td>\n      <td>Andaman and Nicobar Islands</td>\n      <td>7082.0</td>\n      <td>None</td>\n      <td>33.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2020-05-21</td>\n      <td>Andaman and Nicobar Islands</td>\n      <td>7167.0</td>\n      <td>None</td>\n      <td>33.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2020-05-22</td>\n      <td>Andaman and Nicobar Islands</td>\n      <td>7263.0</td>\n      <td>None</td>\n      <td>33.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2020-05-23</td>\n      <td>Andaman and Nicobar Islands</td>\n      <td>7327.0</td>\n      <td>None</td>\n      <td>33.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "         Date                        State  TotalSamples Negative  Positive\n0  2020-04-17  Andaman and Nicobar Islands        1403.0     1210      12.0\n1  2020-04-24  Andaman and Nicobar Islands        2679.0     None      27.0\n2  2020-04-27  Andaman and Nicobar Islands        2848.0     None      33.0\n3  2020-05-01  Andaman and Nicobar Islands        3754.0     None      33.0\n4  2020-05-16  Andaman and Nicobar Islands        6677.0     None      33.0\n5  2020-05-19  Andaman and Nicobar Islands        6965.0     None      33.0\n6  2020-05-20  Andaman and Nicobar Islands        7082.0     None      33.0\n7  2020-05-21  Andaman and Nicobar Islands        7167.0     None      33.0\n8  2020-05-22  Andaman and Nicobar Islands        7263.0     None      33.0\n9  2020-05-23  Andaman and Nicobar Islands        7327.0     None      33.0"}, "execution_count": 23, "metadata": {}, "output_type": "execute_result"}], "source": "# Change the path with your google bucket path\nstate_details = spark.read.load(\"gs://dataproc-staging-asia-east2-441991837520-q2hvd2t0/notebooks/jupyter/StatewiseTestingDetails.csv\",\n                          format=\"csv\", \n                          sep=\",\", \n                          inferSchema=\"true\", \n                          header=\"true\")\n\nstate_details.limit(10).toPandas()"}, {"cell_type": "code", "execution_count": 24, "metadata": {"id": "0QqhjXKjLqEa"}, "outputs": [], "source": "state_details=state_details.withColumnRenamed(\"State\",\"State/UnionTerritory\")"}, {"cell_type": "code", "execution_count": 25, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 363}, "id": "ArgZJuhK-QmQ", "outputId": "1a5cb63b-0da2-49cd-a760-7c505c9e13d7"}, "outputs": [{"data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>State/UnionTerritory</th>\n      <th>Date</th>\n      <th>Time</th>\n      <th>Deaths</th>\n      <th>Date</th>\n      <th>TotalSamples</th>\n      <th>Negative</th>\n      <th>Positive</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Kerala</td>\n      <td>2020-01-30</td>\n      <td>6:00 PM</td>\n      <td>0</td>\n      <td>2021-08-10</td>\n      <td>28745545.0</td>\n      <td>None</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Kerala</td>\n      <td>2020-01-30</td>\n      <td>6:00 PM</td>\n      <td>0</td>\n      <td>2021-08-09</td>\n      <td>28612776.0</td>\n      <td>None</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Kerala</td>\n      <td>2020-01-30</td>\n      <td>6:00 PM</td>\n      <td>0</td>\n      <td>2021-08-08</td>\n      <td>28514136.0</td>\n      <td>None</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Kerala</td>\n      <td>2020-01-30</td>\n      <td>6:00 PM</td>\n      <td>0</td>\n      <td>2021-08-07</td>\n      <td>28379940.0</td>\n      <td>None</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Kerala</td>\n      <td>2020-01-30</td>\n      <td>6:00 PM</td>\n      <td>0</td>\n      <td>2021-08-06</td>\n      <td>28227419.0</td>\n      <td>None</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Kerala</td>\n      <td>2020-01-30</td>\n      <td>6:00 PM</td>\n      <td>0</td>\n      <td>2021-08-05</td>\n      <td>28075527.0</td>\n      <td>None</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Kerala</td>\n      <td>2020-01-30</td>\n      <td>6:00 PM</td>\n      <td>0</td>\n      <td>2021-08-04</td>\n      <td>27912151.0</td>\n      <td>None</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Kerala</td>\n      <td>2020-01-30</td>\n      <td>6:00 PM</td>\n      <td>0</td>\n      <td>2021-08-03</td>\n      <td>27715059.0</td>\n      <td>None</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Kerala</td>\n      <td>2020-01-30</td>\n      <td>6:00 PM</td>\n      <td>0</td>\n      <td>2021-08-02</td>\n      <td>27515603.0</td>\n      <td>None</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Kerala</td>\n      <td>2020-01-30</td>\n      <td>6:00 PM</td>\n      <td>0</td>\n      <td>2021-08-01</td>\n      <td>27387700.0</td>\n      <td>None</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "  State/UnionTerritory        Date     Time  Deaths        Date  TotalSamples  \\\n0               Kerala  2020-01-30  6:00 PM       0  2021-08-10    28745545.0   \n1               Kerala  2020-01-30  6:00 PM       0  2021-08-09    28612776.0   \n2               Kerala  2020-01-30  6:00 PM       0  2021-08-08    28514136.0   \n3               Kerala  2020-01-30  6:00 PM       0  2021-08-07    28379940.0   \n4               Kerala  2020-01-30  6:00 PM       0  2021-08-06    28227419.0   \n5               Kerala  2020-01-30  6:00 PM       0  2021-08-05    28075527.0   \n6               Kerala  2020-01-30  6:00 PM       0  2021-08-04    27912151.0   \n7               Kerala  2020-01-30  6:00 PM       0  2021-08-03    27715059.0   \n8               Kerala  2020-01-30  6:00 PM       0  2021-08-02    27515603.0   \n9               Kerala  2020-01-30  6:00 PM       0  2021-08-01    27387700.0   \n\n  Negative  Positive  \n0     None       NaN  \n1     None       NaN  \n2     None       NaN  \n3     None       NaN  \n4     None       NaN  \n5     None       NaN  \n6     None       NaN  \n7     None       NaN  \n8     None       NaN  \n9     None       NaN  "}, "execution_count": 25, "metadata": {}, "output_type": "execute_result"}], "source": "# Left Join 'Case' with 'State_details' on State Column\ncases = cases.join(state_details, ['State/UnionTerritory'],how='left')\ncases.limit(10).toPandas()"}, {"cell_type": "markdown", "metadata": {"id": "2SKfViYZ-QmQ"}, "source": "### 2. Use SQL with DataFrames"}, {"cell_type": "markdown", "metadata": {"id": "AspuVKXs-QmQ"}, "source": "We first register the cases dataframe to a temporary table cases_table on which we can run SQL operations. As you can see, the result of the SQL select statement is again a Spark Dataframe.\n\nAll complex SQL queries like GROUP BY, HAVING, AND ORDER BY clauses can be applied in 'Sql' function"}, {"cell_type": "code", "execution_count": 26, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "hD-kbRW6-QmQ", "outputId": "92b6ed51-10a0-4e8b-e8a2-85e38d6731a1"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+-------+------+------------+\n|State/UnionTerritory|   Time|Deaths|TotalSamples|\n+--------------------+-------+------+------------+\n|         Maharashtra|5:00 PM|    64| 4.9905065E7|\n|         Maharashtra|5:00 PM|    64| 4.9725694E7|\n|         Maharashtra|5:00 PM|    64| 4.9568519E7|\n|         Maharashtra|5:00 PM|    64| 4.9372212E7|\n|         Maharashtra|5:00 PM|    64| 4.9172531E7|\n|         Maharashtra|5:00 PM|    64| 4.8962106E7|\n|         Maharashtra|5:00 PM|    64| 4.8744201E7|\n|         Maharashtra|5:00 PM|    64| 4.8532523E7|\n|         Maharashtra|5:00 PM|    64| 4.8352467E7|\n|         Maharashtra|5:00 PM|    64|  4.818535E7|\n|         Maharashtra|5:00 PM|    64| 4.7967609E7|\n|         Maharashtra|5:00 PM|    64| 4.7760862E7|\n|         Maharashtra|5:00 PM|    64| 4.7559938E7|\n|         Maharashtra|5:00 PM|    64| 4.7369757E7|\n|         Maharashtra|5:00 PM|    64| 4.7176715E7|\n|         Maharashtra|5:00 PM|    64| 4.6995122E7|\n|         Maharashtra|5:00 PM|    64| 4.6846984E7|\n|         Maharashtra|5:00 PM|    64| 4.6644448E7|\n|         Maharashtra|5:00 PM|    64|  4.644636E7|\n|         Maharashtra|5:00 PM|    64| 4.6264059E7|\n+--------------------+-------+------+------------+\nonly showing top 20 rows\n\n"}], "source": "cases=cases.drop(\"Positive\")\ncases=cases.drop(\"Negative\")\ncases=cases.drop(\"Date\")\ncases.registerTempTable('cases_table')\nsqlcontext=SQLContext(spark)\nnewDF = sqlcontext.sql('select * from cases_table where Deaths > 50')\nnewDF.show()"}, {"cell_type": "markdown", "metadata": {"id": "gM0KIe4D-QmQ"}, "source": "### 3. Create New Columns"}, {"cell_type": "markdown", "metadata": {"id": "zDtoyVAt-QmQ"}, "source": "There are many ways that you can use to create a column in a PySpark Dataframe."}, {"cell_type": "markdown", "metadata": {"id": "Fvb1xKOe-QmQ"}, "source": "#### [1] Using Spark Native Functions"}, {"cell_type": "markdown", "metadata": {"id": "GYWsppYR-QmQ"}, "source": "We can use .withcolumn along with PySpark SQL functions to create a new column. In essence, you can find String functions, Date functions, and Math functions already implemented using Spark functions. Our first function, the F.col function gives us access to the column. So if we wanted to add 100 to a column, we could use F.col as:"}, {"cell_type": "code", "execution_count": 27, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "Uyd8hXTK-QmR", "outputId": "3354f9a1-dda4-44b5-948b-9bc2e9414e58"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+-------+------+------------+----------+\n|State/UnionTerritory|   Time|Deaths|TotalSamples|New_Deaths|\n+--------------------+-------+------+------------+----------+\n|              Kerala|6:00 PM|     0| 2.8745545E7|        10|\n|              Kerala|6:00 PM|     0| 2.8612776E7|        10|\n|              Kerala|6:00 PM|     0| 2.8514136E7|        10|\n|              Kerala|6:00 PM|     0|  2.837994E7|        10|\n|              Kerala|6:00 PM|     0| 2.8227419E7|        10|\n|              Kerala|6:00 PM|     0| 2.8075527E7|        10|\n|              Kerala|6:00 PM|     0| 2.7912151E7|        10|\n|              Kerala|6:00 PM|     0| 2.7715059E7|        10|\n|              Kerala|6:00 PM|     0| 2.7515603E7|        10|\n|              Kerala|6:00 PM|     0|   2.73877E7|        10|\n|              Kerala|6:00 PM|     0|  2.721701E7|        10|\n|              Kerala|6:00 PM|     0| 2.7049431E7|        10|\n|              Kerala|6:00 PM|     0| 2.6896792E7|        10|\n|              Kerala|6:00 PM|     0| 2.6733694E7|        10|\n|              Kerala|6:00 PM|     0| 2.6536792E7|        10|\n|              Kerala|6:00 PM|     0| 2.6357662E7|        10|\n|              Kerala|6:00 PM|     0|  2.624828E7|        10|\n|              Kerala|6:00 PM|     0| 2.6106272E7|        10|\n|              Kerala|6:00 PM|     0| 2.5950704E7|        10|\n|              Kerala|6:00 PM|     0| 2.5822215E7|        10|\n+--------------------+-------+------+------------+----------+\nonly showing top 20 rows\n\n"}], "source": "import pyspark.sql.functions as F\n\ncasesWithNewConfirmed = cases.withColumn(\"New_Deaths\", 10 + F.col(\"Deaths\"))\ncasesWithNewConfirmed.show()"}, {"cell_type": "markdown", "metadata": {"id": "-6i-CFlZ-QmR"}, "source": "We can also use math functions like F.exp function:"}, {"cell_type": "code", "execution_count": 28, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "RinlzrrK-QmR", "outputId": "a54a09bf-fc24-4a2d-8ef8-865d77f4f0cc"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+-------+------+------------+------------+\n|State/UnionTerritory|   Time|Deaths|TotalSamples|ExpConfirmed|\n+--------------------+-------+------+------------+------------+\n|              Kerala|6:00 PM|     0| 2.8745545E7|         1.0|\n|              Kerala|6:00 PM|     0| 2.8612776E7|         1.0|\n|              Kerala|6:00 PM|     0| 2.8514136E7|         1.0|\n|              Kerala|6:00 PM|     0|  2.837994E7|         1.0|\n|              Kerala|6:00 PM|     0| 2.8227419E7|         1.0|\n|              Kerala|6:00 PM|     0| 2.8075527E7|         1.0|\n|              Kerala|6:00 PM|     0| 2.7912151E7|         1.0|\n|              Kerala|6:00 PM|     0| 2.7715059E7|         1.0|\n|              Kerala|6:00 PM|     0| 2.7515603E7|         1.0|\n|              Kerala|6:00 PM|     0|   2.73877E7|         1.0|\n|              Kerala|6:00 PM|     0|  2.721701E7|         1.0|\n|              Kerala|6:00 PM|     0| 2.7049431E7|         1.0|\n|              Kerala|6:00 PM|     0| 2.6896792E7|         1.0|\n|              Kerala|6:00 PM|     0| 2.6733694E7|         1.0|\n|              Kerala|6:00 PM|     0| 2.6536792E7|         1.0|\n|              Kerala|6:00 PM|     0| 2.6357662E7|         1.0|\n|              Kerala|6:00 PM|     0|  2.624828E7|         1.0|\n|              Kerala|6:00 PM|     0| 2.6106272E7|         1.0|\n|              Kerala|6:00 PM|     0| 2.5950704E7|         1.0|\n|              Kerala|6:00 PM|     0| 2.5822215E7|         1.0|\n+--------------------+-------+------+------------+------------+\nonly showing top 20 rows\n\n"}], "source": "casesWithExpConfirmed = cases.withColumn(\"ExpConfirmed\", F.exp(\"Deaths\"))\ncasesWithExpConfirmed.show()"}, {"cell_type": "markdown", "metadata": {"id": "x5EZz51c-QmR"}, "source": "#### [2] Using Spark UDFs"}, {"cell_type": "markdown", "metadata": {"id": "HQtn4tFf-QmR"}, "source": "Sometimes we want to do complicated things to a column or multiple columns. This could be thought of as a map operation on a PySpark Dataframe to a single column or multiple columns. While Spark SQL functions do solve many use cases when it comes to column creation, I use Spark UDF whenever I need more matured Python functionality. \\\n\nTo use Spark UDFs, we need to use the F.udf function to convert a regular python function to a Spark UDF. We also need to specify the return type of the function. In this example the return type is StringType()"}, {"cell_type": "code", "execution_count": 29, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "MVhXalGl-QmR", "outputId": "06de9b06-45a3-47f8-910c-993cad7c0d0c"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+-------+------+------------+-------+\n|State/UnionTerritory|   Time|Deaths|TotalSamples|HighLow|\n+--------------------+-------+------+------------+-------+\n|              Kerala|6:00 PM|     0| 2.8745545E7|    low|\n|              Kerala|6:00 PM|     0| 2.8612776E7|    low|\n|              Kerala|6:00 PM|     0| 2.8514136E7|    low|\n|              Kerala|6:00 PM|     0|  2.837994E7|    low|\n|              Kerala|6:00 PM|     0| 2.8227419E7|    low|\n|              Kerala|6:00 PM|     0| 2.8075527E7|    low|\n|              Kerala|6:00 PM|     0| 2.7912151E7|    low|\n|              Kerala|6:00 PM|     0| 2.7715059E7|    low|\n|              Kerala|6:00 PM|     0| 2.7515603E7|    low|\n|              Kerala|6:00 PM|     0|   2.73877E7|    low|\n|              Kerala|6:00 PM|     0|  2.721701E7|    low|\n|              Kerala|6:00 PM|     0| 2.7049431E7|    low|\n|              Kerala|6:00 PM|     0| 2.6896792E7|    low|\n|              Kerala|6:00 PM|     0| 2.6733694E7|    low|\n|              Kerala|6:00 PM|     0| 2.6536792E7|    low|\n|              Kerala|6:00 PM|     0| 2.6357662E7|    low|\n|              Kerala|6:00 PM|     0|  2.624828E7|    low|\n|              Kerala|6:00 PM|     0| 2.6106272E7|    low|\n|              Kerala|6:00 PM|     0| 2.5950704E7|    low|\n|              Kerala|6:00 PM|     0| 2.5822215E7|    low|\n+--------------------+-------+------+------------+-------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "Traceback (most recent call last):                                              \n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 643, in main\n    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n    raise EOFError\nEOFError\n"}], "source": "import pyspark.sql.functions as F\nfrom pyspark.sql.types import *\n\ndef casesHighLow(Deaths):\n    if Deaths < 10: \n        return 'low'\n    else:\n        return 'high'\n    \n#convert to a UDF Function by passing in the function and return type of function\ncasesHighLowUDF = F.udf(casesHighLow, StringType())\nCasesWithHighLow = cases.withColumn(\"HighLow\", casesHighLowUDF(\"Deaths\"))\nCasesWithHighLow.show()"}, {"cell_type": "markdown", "metadata": {"id": "d6N9KM_w-QmR"}, "source": "#### [3] Using Pandas UDF"}, {"cell_type": "markdown", "metadata": {"id": "s6yIWTYa-QmR"}, "source": "This allows you to use pandas functionality with Spark. I generally use it when I have to run a groupBy operation on a Spark dataframe or whenever I need to create rolling features\n \nThe way we use it is by using the F.pandas_udf decorator. **We assume here that the input to the function will be a pandas data frame**\n\nThe only complexity here is that we have to provide a schema for the output Dataframe. We can use the original schema of a dataframe to create the outSchema."}, {"cell_type": "code", "execution_count": 30, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "PFqXM8Pc-QmS", "outputId": "ff40d0c9-0cf1-48e6-ead6-b2e0c5be2cb5"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- State/UnionTerritory: string (nullable = true)\n |-- Time: string (nullable = true)\n |-- Deaths: integer (nullable = true)\n |-- TotalSamples: double (nullable = true)\n\n"}], "source": "cases.printSchema()"}, {"cell_type": "code", "execution_count": 31, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 261}, "id": "ZsB0utsg-QmS", "outputId": "c885592a-031f-4a64-a78a-4404f73e195f"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>State/UnionTerritory</th>\n      <th>Time</th>\n      <th>Deaths</th>\n      <th>TotalSamples</th>\n      <th>normalized_deaths</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Chhattisgarh</td>\n      <td>6:00 PM</td>\n      <td>0</td>\n      <td>11762041.0</td>\n      <td>-4038.982387</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Chhattisgarh</td>\n      <td>6:00 PM</td>\n      <td>0</td>\n      <td>11720007.0</td>\n      <td>-4038.982387</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Chhattisgarh</td>\n      <td>6:00 PM</td>\n      <td>0</td>\n      <td>11692078.0</td>\n      <td>-4038.982387</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Chhattisgarh</td>\n      <td>6:00 PM</td>\n      <td>0</td>\n      <td>11666597.0</td>\n      <td>-4038.982387</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Chhattisgarh</td>\n      <td>6:00 PM</td>\n      <td>0</td>\n      <td>11624064.0</td>\n      <td>-4038.982387</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "  State/UnionTerritory     Time  Deaths  TotalSamples  normalized_deaths\n0         Chhattisgarh  6:00 PM       0    11762041.0       -4038.982387\n1         Chhattisgarh  6:00 PM       0    11720007.0       -4038.982387\n2         Chhattisgarh  6:00 PM       0    11692078.0       -4038.982387\n3         Chhattisgarh  6:00 PM       0    11666597.0       -4038.982387\n4         Chhattisgarh  6:00 PM       0    11624064.0       -4038.982387"}, "execution_count": 31, "metadata": {}, "output_type": "execute_result"}], "source": "from pyspark.sql.types import IntegerType, StringType, DoubleType, BooleanType\nfrom pyspark.sql.types import StructType, StructField\n\n# Declare the schema for the output of our function\n\noutSchema = StructType([StructField('State/UnionTerritory',StringType(),True),\n                        StructField('Time',StringType(),True),\n                        StructField('Deaths',IntegerType(),True),\n                        StructField('TotalSamples',DoubleType(),True),\n                        StructField('normalized_deaths',DoubleType(),True)\n                       ])\n# decorate our function with pandas_udf decorator\n@F.pandas_udf(outSchema, F.PandasUDFType.GROUPED_MAP)\ndef subtract_mean(pdf):\n    # pdf is a pandas.DataFrame\n    v = pdf.Deaths\n    v = v - v.mean()\n    pdf['normalized_deaths'] = v\n    return pdf\n\nconfirmed_groupwise_normalization = cases.groupby(\"State/UnionTerritory\").apply(subtract_mean)\n\nconfirmed_groupwise_normalization.limit(5).toPandas()"}, {"cell_type": "markdown", "metadata": {"id": "iAOErk0m-QmS"}, "source": "### 4. Spark Window Functions"}, {"cell_type": "markdown", "metadata": {"id": "yLl_u-Q6-QmS"}, "source": "We will simply look at some of the most important and useful window functions available."}, {"cell_type": "code", "execution_count": 35, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "v69rPWL9-QmS", "outputId": "eedc3973-4b7c-49fc-ed28-ade28f72c9da"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+--------------------+------------+--------+--------+\n|      Date|               State|TotalSamples|Negative|Positive|\n+----------+--------------------+------------+--------+--------+\n|2020-04-17|Andaman and Nicob...|      1403.0|    1210|    12.0|\n|2020-04-24|Andaman and Nicob...|      2679.0|    null|    27.0|\n|2020-04-27|Andaman and Nicob...|      2848.0|    null|    33.0|\n|2020-05-01|Andaman and Nicob...|      3754.0|    null|    33.0|\n|2020-05-16|Andaman and Nicob...|      6677.0|    null|    33.0|\n|2020-05-19|Andaman and Nicob...|      6965.0|    null|    33.0|\n|2020-05-20|Andaman and Nicob...|      7082.0|    null|    33.0|\n|2020-05-21|Andaman and Nicob...|      7167.0|    null|    33.0|\n|2020-05-22|Andaman and Nicob...|      7263.0|    null|    33.0|\n|2020-05-23|Andaman and Nicob...|      7327.0|    null|    33.0|\n|2020-05-24|Andaman and Nicob...|      7327.0|    null|    33.0|\n|2020-05-25|Andaman and Nicob...|      7363.0|    null|    33.0|\n|2020-05-26|Andaman and Nicob...|      7448.0|    null|    33.0|\n|2020-05-27|Andaman and Nicob...|      7499.0|    null|    33.0|\n|2020-05-28|Andaman and Nicob...|      7519.0|    null|    33.0|\n|2020-05-29|Andaman and Nicob...|      7567.0|    null|    33.0|\n|2020-05-30|Andaman and Nicob...|      7567.0|    null|    33.0|\n|2020-05-31|Andaman and Nicob...|      7706.0|    null|    33.0|\n|2020-06-01|Andaman and Nicob...|      7805.0|    null|    33.0|\n|2020-06-02|Andaman and Nicob...|      8086.0|    null|    33.0|\n+----------+--------------------+------------+--------+--------+\nonly showing top 20 rows\n\n"}], "source": "# Change the path with your google bucket path\nstate_date_time = spark.read.load(\"gs://dataproc-staging-asia-east2-441991837520-q2hvd2t0/notebooks/jupyter/StatewiseTestingDetails.csv\",\n                          format=\"csv\", \n                          sep=\",\", \n                          inferSchema=\"true\", \n                          header=\"true\")\n\nstate_date_time.show()"}, {"cell_type": "markdown", "metadata": {"id": "J16nuBTt-QmS"}, "source": "#### Ranking"}, {"cell_type": "markdown", "metadata": {"id": "KhBDrUw--QmS"}, "source": "You can get rank as well as dense_rank on a group using this function. For example, you may want to have a column in your cases table that provides the rank of infection_case based on the number of infection_case in a province. We can do this by:"}, {"cell_type": "code", "execution_count": 36, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "IWfmY93A-QmS", "outputId": "f6f12b5d-bc29-419a-facd-606317607391"}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 49:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------+------+------------+----+\n|State/UnionTerritory|   Time|Deaths|TotalSamples|rank|\n+--------------------+-------+------+------------+----+\n|        Chhattisgarh|8:00 AM| 13544| 1.1762041E7|   1|\n|        Chhattisgarh|8:00 AM| 13544| 1.1720007E7|   1|\n|        Chhattisgarh|8:00 AM| 13544| 1.1692078E7|   1|\n|        Chhattisgarh|8:00 AM| 13544| 1.1666597E7|   1|\n|        Chhattisgarh|8:00 AM| 13544| 1.1624064E7|   1|\n|        Chhattisgarh|8:00 AM| 13544| 1.1580254E7|   1|\n|        Chhattisgarh|8:00 AM| 13544| 1.1535855E7|   1|\n|        Chhattisgarh|8:00 AM| 13544| 1.1493309E7|   1|\n|        Chhattisgarh|8:00 AM| 13544| 1.1452754E7|   1|\n|        Chhattisgarh|8:00 AM| 13544| 1.1416645E7|   1|\n|        Chhattisgarh|8:00 AM| 13544| 1.1394233E7|   1|\n|        Chhattisgarh|8:00 AM| 13544| 1.1355589E7|   1|\n|        Chhattisgarh|8:00 AM| 13544| 1.1312875E7|   1|\n|        Chhattisgarh|8:00 AM| 13544| 1.1272374E7|   1|\n|        Chhattisgarh|8:00 AM| 13544| 1.1236166E7|   1|\n|        Chhattisgarh|8:00 AM| 13544| 1.1197168E7|   1|\n|        Chhattisgarh|8:00 AM| 13544| 1.1159728E7|   1|\n|        Chhattisgarh|8:00 AM| 13544| 1.1133888E7|   1|\n|        Chhattisgarh|8:00 AM| 13544| 1.1100262E7|   1|\n|        Chhattisgarh|8:00 AM| 13544| 1.1069812E7|   1|\n+--------------------+-------+------+------------+----+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.window import Window\nwindowSpec = Window().partitionBy(['State/UnionTerritory']).orderBy(F.desc('Deaths'))\ncases.withColumn(\"rank\",F.rank().over(windowSpec)).show()"}, {"cell_type": "markdown", "metadata": {"id": "thbo75hn-QmV"}, "source": "### 5. Close Spark Instance"}, {"cell_type": "code", "execution_count": 43, "metadata": {"id": "rG8gI6yx-QmV"}, "outputs": [], "source": "spark.stop()"}], "metadata": {"colab": {"collapsed_sections": [], "name": "Copy_of_PySpark_Dataframe_Complete_Guide_(with_COVID_19_Dataset).ipynb", "provenance": []}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.13"}}, "nbformat": 4, "nbformat_minor": 4}