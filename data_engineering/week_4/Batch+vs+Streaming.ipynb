{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd4eed60-d7fd-4ab2-8ef7-ba201f4ef7f2",
   "metadata": {},
   "source": [
    "# Batch vs. Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "700758ef-04d5-44db-96d8-be4b447487b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's first use an API to get the data\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "\n",
    "url = \"https://api.alternative.me/v2/ticker/bitcoin/?convert=USD\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3605091-ae42-43f6-8f2a-fce4405e5a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_json(url):\n",
    "    sess = requests.Session()\n",
    "    with sess.get(url, headers = None, stream = True) as response:\n",
    "        with open('data.json', 'w+') as f:\n",
    "            json.dump(response.json(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b14b993e-c6c9-4b0d-b9ac-7c01341f02a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stream(url):\n",
    "    sess = requests.Session()\n",
    "    with sess.get(url, headers = None, stream = True) as response:\n",
    "        with open('data.json', 'r+') as f:\n",
    "            data = json.load(f)\n",
    "            if type(data) is list:\n",
    "                data = data\n",
    "            else:\n",
    "                data = [data]\n",
    "        data.append(response.json())\n",
    "        with open('data.json', 'w+') as f:\n",
    "            json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e0dad2-5675-4609-a5a9-4d11acfee236",
   "metadata": {},
   "source": [
    "These two functions allow us to create a JSON file (Initialize the file), and then we can update every 10 seconds. Let's start with a batch: let's populate the JSON file with 10 min of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bddb279-f7be-48a9-8328-8502060a44ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "initiate_json(url)\n",
    "for i in range(60):\n",
    "    get_stream(url)\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be903dbe-bfde-4cb5-9523-4d42cd7117c9",
   "metadata": {},
   "source": [
    "# Batch\n",
    "\n",
    "As described before, we have been doing batch processing over the last 4 weeks. Let's run Batch, which will be very similar to before. We want to extract the BTC prices and the total supply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02b9ff90-6cd7-43eb-bb80-c871e667704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's start by setting up the imports and the create the Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType, TimestampType\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"BatchJob\")\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .config(\"spark.ui.port\",\"42229\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f422a01f-6965-4f84-a5f8-8f56f0c4c09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                data|            metadata|\n",
      "+--------------------+--------------------+\n",
      "|{{19046825, 1, 16...|{null, 3106, 1653...|\n",
      "|{{19046825, 1, 16...|{null, 3106, 1653...|\n",
      "|{{19046825, 1, 16...|{null, 3106, 1653...|\n",
      "|{{19046825, 1, 16...|{null, 3106, 1653...|\n",
      "|{{19046825, 1, 16...|{null, 3106, 1653...|\n",
      "|{{19046825, 1, 16...|{null, 3106, 1653...|\n",
      "|{{19046825, 1, 16...|{null, 3106, 1653...|\n",
      "|{{19046825, 1, 16...|{null, 3106, 1653...|\n",
      "|{{19046825, 1, 16...|{null, 3106, 1653...|\n",
      "|{{19046825, 1, 16...|{null, 3106, 1653...|\n",
      "|{{19046825, 1, 16...|{null, 3106, 1653...|\n",
      "|{{19046825, 1, 16...|{null, 3106, 1653...|\n",
      "|{{19046825, 1, 16...|{null, 3106, 1653...|\n",
      "|{{19046825, 1, 16...|{null, 3106, 1653...|\n",
      "|{{19046825, 1, 16...|{null, 3106, 1653...|\n",
      "|{{19046825, 1, 16...|{null, 3106, 1653...|\n",
      "|{{19046825, 1, 16...|{null, 3106, 1653...|\n",
      "|{{19046825, 1, 16...|{null, 3106, 1653...|\n",
      "|{{19046825, 1, 16...|{null, 3106, 1653...|\n",
      "|{{19046825, 1, 16...|{null, 3106, 1653...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Let's work now with Schemas, the first step is to create a Schema to use for our data\n",
    "def btc_schema():\n",
    "    return StructType([\n",
    "        StructField(\"Date\", TimestampType(), True),\n",
    "        StructField(\"Price\", FloatType(), True),\n",
    "        StructField(\"Supply\", IntegerType(), True),\n",
    "    ])\n",
    "\n",
    "\n",
    "## Let's read the data from the json file\n",
    "raw_events = spark \\\n",
    "    .read \\\n",
    "    .json('file:///media/data/data.json')\n",
    "\n",
    "\n",
    "raw_events.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04a1160-cff3-4299-888e-06aa56c8de73",
   "metadata": {},
   "source": [
    "As we can observe, the data has a weird structure, where we have both data and metadata. We need to unroll the JSON file to extract the correct information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad569d5f-2c1a-488f-8afe-3c41fbee9650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(data=Row(1=Row(circulating_supply=19046825, id=1, last_updated=1653265752, max_supply=21000000, name='Bitcoin', quotes=Row(USD=Row(market_cap=578216982523, percent_change_1h=-0.460638268084277, percent_change_24h=2.73122435604924, percent_change_7d=-3.21842635088867, percentage_change_1h=-0.460638268084277, percentage_change_24h=2.73122435604924, percentage_change_7d=-3.21842635088867, price=30311.0, volume_24h=17443986056)), rank=1, symbol='BTC', total_supply=19046825, website_slug='bitcoin')))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "btc_eda = raw_events\\\n",
    "        .select(raw_events.data)\n",
    "\n",
    "btc_table.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549f6fd3-678f-40b7-9577-ce488feef702",
   "metadata": {},
   "source": [
    "Using the `take()` method, we are able to understand exactly the data. We will now extract the information needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b28c390-0e91-4fdb-8f0a-a44f15a4e562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+--------+\n",
      "|               Date|  Price|  Supply|\n",
      "+-------------------+-------+--------+\n",
      "|2022-05-23 00:29:12|30311.0|19046825|\n",
      "|2022-05-23 00:29:12|30311.0|19046825|\n",
      "|2022-05-23 00:29:12|30311.0|19046825|\n",
      "|2022-05-23 00:29:12|30311.0|19046825|\n",
      "|2022-05-23 00:29:12|30311.0|19046825|\n",
      "|2022-05-23 00:29:12|30311.0|19046825|\n",
      "|2022-05-23 00:29:12|30311.0|19046825|\n",
      "|2022-05-23 00:29:12|30311.0|19046825|\n",
      "|2022-05-23 00:29:12|30311.0|19046825|\n",
      "|2022-05-23 00:29:12|30311.0|19046825|\n",
      "|2022-05-23 00:29:12|30311.0|19046825|\n",
      "|2022-05-23 00:29:12|30311.0|19046825|\n",
      "|2022-05-23 00:29:12|30311.0|19046825|\n",
      "|2022-05-23 00:29:12|30311.0|19046825|\n",
      "|2022-05-23 00:29:12|30311.0|19046825|\n",
      "|2022-05-23 00:29:12|30311.0|19046825|\n",
      "|2022-05-23 00:29:12|30311.0|19046825|\n",
      "|2022-05-23 00:29:12|30311.0|19046825|\n",
      "|2022-05-23 00:29:12|30311.0|19046825|\n",
      "|2022-05-23 00:29:12|30311.0|19046825|\n",
      "|2022-05-23 00:29:12|30311.0|19046825|\n",
      "|2022-05-23 00:29:12|30311.0|19046825|\n",
      "|2022-05-23 00:29:12|30311.0|19046825|\n",
      "|2022-05-23 00:29:12|30311.0|19046825|\n",
      "|2022-05-23 00:29:12|30311.0|19046825|\n",
      "|2022-05-23 00:29:12|30311.0|19046825|\n",
      "|2022-05-23 00:29:12|30311.0|19046825|\n",
      "|2022-05-23 00:29:12|30311.0|19046825|\n",
      "|2022-05-23 00:29:12|30311.0|19046825|\n",
      "|2022-05-23 00:29:12|30311.0|19046825|\n",
      "|2022-05-23 00:34:24|30301.0|19046825|\n",
      "|2022-05-23 00:34:24|30301.0|19046825|\n",
      "|2022-05-23 00:34:24|30301.0|19046825|\n",
      "|2022-05-23 00:34:24|30301.0|19046825|\n",
      "|2022-05-23 00:34:24|30301.0|19046825|\n",
      "|2022-05-23 00:34:24|30301.0|19046825|\n",
      "|2022-05-23 00:34:24|30301.0|19046825|\n",
      "|2022-05-23 00:34:24|30301.0|19046825|\n",
      "|2022-05-23 00:34:24|30301.0|19046825|\n",
      "|2022-05-23 00:34:24|30301.0|19046825|\n",
      "|2022-05-23 00:34:24|30301.0|19046825|\n",
      "|2022-05-23 00:34:24|30301.0|19046825|\n",
      "|2022-05-23 00:34:24|30301.0|19046825|\n",
      "|2022-05-23 00:34:24|30301.0|19046825|\n",
      "|2022-05-23 00:34:24|30301.0|19046825|\n",
      "|2022-05-23 00:34:24|30301.0|19046825|\n",
      "|2022-05-23 00:34:24|30301.0|19046825|\n",
      "|2022-05-23 00:34:24|30301.0|19046825|\n",
      "|2022-05-23 00:34:24|30301.0|19046825|\n",
      "|2022-05-23 00:34:24|30301.0|19046825|\n",
      "|2022-05-23 00:34:24|30301.0|19046825|\n",
      "|2022-05-23 00:34:24|30301.0|19046825|\n",
      "|2022-05-23 00:34:24|30301.0|19046825|\n",
      "|2022-05-23 00:34:24|30301.0|19046825|\n",
      "|2022-05-23 00:34:24|30301.0|19046825|\n",
      "|2022-05-23 00:34:24|30301.0|19046825|\n",
      "|2022-05-23 00:34:24|30301.0|19046825|\n",
      "|2022-05-23 00:34:24|30301.0|19046825|\n",
      "|2022-05-23 00:34:24|30301.0|19046825|\n",
      "|2022-05-23 00:38:42|30319.0|19046825|\n",
      "+-------------------+-------+--------+\n",
      "only showing top 60 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "btc_table = raw_events\\\n",
    "        .select(raw_events['data']['1']['last_updated'].cast(\"timestamp\").alias('Date'), \n",
    "                raw_events['data']['1']['quotes']['USD']['price'].cast(\"float\").alias('Price'), \n",
    "                raw_events['data']['1']['total_supply'].cast(\"int\").alias('Supply'))\n",
    "\n",
    "btc_table.show(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38ddf364-3c1f-49dc-bd07-f3c34ce47b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we can write the table as a parquet file\n",
    "btc_table \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet('file:///media/data/btc.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afee701-920b-4d9a-a9ec-5ea2b7d893a9",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "\n",
    "So, what changes if we are streaming? Data is coming in a stream process! I will create a script to populate a new JSON file without a stop to observe what changes in stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6b73b1-ad21-4861-af94-ffe8abd0cc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Price: float (nullable = true)\n",
      " |-- Supply: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/23 04:31:58 WARN org.apache.spark.sql.streaming.StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-0d77a75b-eee0-4bac-bbe5-5373931a1bb7. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/05/23 04:31:58 WARN org.apache.spark.sql.streaming.StreamingQueryManager: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|Current_Price|Avg_Price|\n",
      "+-------------+---------+\n",
      "|         null|     null|\n",
      "+-------------+---------+\n",
      "\n",
      "+-------------+---------+\n",
      "|Current_Price|Avg_Price|\n",
      "+-------------+---------+\n",
      "|      30187.0|  30177.2|\n",
      "+-------------+---------+\n",
      "\n",
      "+-------------+---------+\n",
      "|Current_Price|Avg_Price|\n",
      "+-------------+---------+\n",
      "|      30187.0|  30177.2|\n",
      "+-------------+---------+\n",
      "\n",
      "+-------------+---------+\n",
      "|Current_Price|Avg_Price|\n",
      "+-------------+---------+\n",
      "|      30187.0|  30177.2|\n",
      "+-------------+---------+\n",
      "\n",
      "+-------------+---------+\n",
      "|Current_Price|Avg_Price|\n",
      "+-------------+---------+\n",
      "|      30187.0|  30177.2|\n",
      "+-------------+---------+\n",
      "\n",
      "+-------------+---------+\n",
      "|Current_Price|Avg_Price|\n",
      "+-------------+---------+\n",
      "|      30187.0|  30177.2|\n",
      "+-------------+---------+\n",
      "\n",
      "+-------------+---------+\n",
      "|Current_Price|Avg_Price|\n",
      "+-------------+---------+\n",
      "|      30187.0|  30177.2|\n",
      "+-------------+---------+\n",
      "\n",
      "+-------------+---------+\n",
      "|Current_Price|Avg_Price|\n",
      "+-------------+---------+\n",
      "|      30187.0|  30177.2|\n",
      "+-------------+---------+\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselect last(Price) as Current_Price, avg(Price) as Avg_Price from btc_price\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 28\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Get the schema of the file\n",
    "raw_schema = spark.read.json(\"file:///media/data_stream.json\").schema\n",
    "\n",
    "raw_events = spark \\\n",
    "    .readStream \\\n",
    "    .format('json') \\\n",
    "    .schema(raw_schema) \\\n",
    "    .json('file:///media/data_stream*.json')\n",
    "\n",
    "## Create the data that we need\n",
    "btc_table = raw_events\\\n",
    "        .select(raw_events['data']['1']['last_updated'].cast(\"timestamp\").alias('Date'), \n",
    "                raw_events['data']['1']['quotes']['USD']['price'].cast(\"float\").alias('Price'), \n",
    "                raw_events['data']['1']['total_supply'].cast(\"int\").alias('Supply'))\n",
    "\n",
    "btc_table.printSchema()\n",
    "\n",
    "## Writing the data in a stream job\n",
    "sink = btc_table \\\n",
    "        .writeStream \\\n",
    "        .queryName(\"btc_price\") \\\n",
    "        .format('memory') \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .start()\n",
    "\n",
    "while True:\n",
    "    spark.sql(\"select last(Price) as Current_Price, avg(Price) as Avg_Price from btc_price\").show()\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40240fa-e7e8-4621-b729-30dbe69c3d19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
