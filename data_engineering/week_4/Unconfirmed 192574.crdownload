#!/usr/bin/env python
"""Extract events from kafka and write them to hdfs
"""
import json
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, from_json
from pyspark.sql.types import StructType, StructField, StringType
import time

def main():
    spark = SparkSession\
        .builder\
        .appName("StreamJob")\
        .master("local[*]")\
        .config("spark.ui.port","42229")\
        .getOrCreate()
    
    ## Get the schema of the file
    raw_schema = spark.read.json("file:///media/data_stream.json").schema

    raw_events = spark \
        .readStream \
        .format('json') \
        .schema(raw_schema) \
        .json('file:///media/data_stream*.json')
    
    ## Create the data that we need
    btc_table = raw_events\
            .select(raw_events['data']['1']['last_updated'].cast("timestamp").alias('Date'), 
                    raw_events['data']['1']['quotes']['USD']['price'].cast("float").alias('Price'), 
                    raw_events['data']['1']['total_supply'].cast("int").alias('Supply'))

    btc_table.printSchema()
    
    ## Writing the data in a stream job
    sink = btc_table \
            .writeStream \
            .queryName("btc_stream") \
            .format('memory') \
            .outputMode("append") \
            .start()

    while True:
        spark.sql("select last(Price) as Current_Price, avg(Price) as Avg_Price from btc_stream").show()
        time.sleep(10)
        
    
if __name__ == "__main__":
    main()